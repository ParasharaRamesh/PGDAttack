{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# About\n",
    "\n",
    "This notebook attempts to finetune a resnet model to be more Robust by leveraging Projected Gradient Descent (PGD) in the two different ways:\n",
    "1. include it as a part of the dataset\n",
    "2. include the projected gradient as well while training along with the regular gradient from the loss function "
   ],
   "id": "9befb1ddab3e4144"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 0. Importing required libraries\n",
   "id": "876d5185378e4464"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-01T08:59:53.325867Z",
     "start_time": "2024-09-01T08:59:53.293844Z"
    }
   },
   "source": [
    "import random\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from torchmetrics import Accuracy\n",
    "from tqdm import tqdm"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1. Setup",
   "id": "5f6951e549aa8a2c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-01T08:25:21.480249Z",
     "start_time": "2024-09-01T08:25:19.936232Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Manual seed for reproducibility\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Params for PGD\n",
    "ALPHA = 2/255\n",
    "STEPS = 20\n",
    "EPSILON = 8/255\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "id": "8e43067e9bcd293c",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1.1 Loading the model",
   "id": "ecc4c4b8b915da80"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-01T08:25:21.711247Z",
     "start_time": "2024-09-01T08:25:21.480249Z"
    }
   },
   "cell_type": "code",
   "source": [
    "weights = ResNet50_Weights.DEFAULT\n",
    "model = resnet50(weights=weights)\n",
    "preprocess = weights.transforms()"
   ],
   "id": "2ff42231a70a45ad",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1.2 Loading the datasets",
   "id": "547319e9ec62f2d9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-01T08:25:21.729639Z",
     "start_time": "2024-09-01T08:25:21.711247Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess_img(example):\n",
    "    example['image'] = preprocess(example['image'])\n",
    "    return example\n",
    "\n",
    "def get_dataloader(split: str, batch_num: int, batch_size: int):\n",
    "    '''\n",
    "        \n",
    "    :param split: can be either train, test or validation \n",
    "    :return: \n",
    "    '''\n",
    "    print(f\"Loading {split} ILSVRC/imagenet-1k dataset...\")\n",
    "    ds = load_dataset(\"ILSVRC/imagenet-1k\", split=split, streaming=True, trust_remote_code=True)\n",
    "    \n",
    "    # Filter out grayscale images\n",
    "    ds = ds.filter(lambda example: example['image'].mode == 'RGB')\n",
    "    \n",
    "    # Preprocess function will be applied to images on-the-fly whenever they are being accessed in the loop\n",
    "    ds = ds.map(preprocess_img)\n",
    "    ds = ds.shuffle(seed=SEED)\n",
    "    \n",
    "    # Only take desired portion of dataset\n",
    "    ds = ds.take(batch_num * batch_size)\n",
    "    print(f\"Creating dataloader with {batch_num} batches for split {split} each with size {batch_size}\")\n",
    "    return DataLoader(ds, batch_size=batch_size)\n",
    "    "
   ],
   "id": "1e9fafc8606bddd2",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1.3 My Resnet PGD Attacker",
   "id": "2c84e145464783a6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-01T08:25:21.745743Z",
     "start_time": "2024-09-01T08:25:21.729639Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ResnetPGDAttacker:\n",
    "    def __init__(self, model, dataloader: DataLoader):\n",
    "        '''\n",
    "        The PGD attack on Resnet model.\n",
    "        :param model: The resnet model on which we perform the attack\n",
    "        :param dataloader: The dataloader loading the input data on which we perform the attack\n",
    "        '''\n",
    "        self.model = model\n",
    "        self.dataloader = dataloader\n",
    "        self.batch_size = dataloader.batch_size\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.adv_images = []\n",
    "        self.labels = []\n",
    "        self.eps = 0\n",
    "        self.alpha = 0\n",
    "        self.steps = 0\n",
    "        self.acc = 0\n",
    "        self.adv_acc = 0\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "        # Nullify gradient for model params\n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def pgd_attack(self, image, label, eps=None, alpha=None, steps=None):\n",
    "        '''\n",
    "        Create adversarial images for given batch of images and labels\n",
    "\n",
    "        :param image: Batch of input images on which we perform the attack, size (BATCH_SIZE, 3, 224, 224)\n",
    "        :param label: Batch of input labels on which we perform the attack, size (BATCH_SIZE)\n",
    "        :return: Adversarial images for the given input images\n",
    "        '''\n",
    "        if eps is None:\n",
    "            eps = self.eps\n",
    "        if alpha is None:\n",
    "            alpha = self.alpha\n",
    "        if steps is None:\n",
    "            steps = self.steps\n",
    "\n",
    "        images = image.clone().detach().to(self.device)\n",
    "        adv_images = images.clone()\n",
    "        labels = label.clone().detach().to(self.device)\n",
    "\n",
    "        # Starting at a uniformly random point within the eps ball\n",
    "        random_noise = torch.zeros_like(adv_images).uniform_(-eps, eps)\n",
    "        adv_images = adv_images + random_noise\n",
    "\n",
    "        for _ in range(steps):\n",
    "            # Enable gradient tracking for adversarial images\n",
    "            adv_images.requires_grad = True\n",
    "\n",
    "            # Get model predictions and apply softmax\n",
    "            outputs = self.model(adv_images).softmax(1)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = self.loss_fn(outputs, labels)\n",
    "\n",
    "            # Compute gradient wrt images\n",
    "            grad = torch.autograd.grad(\n",
    "                loss, adv_images, retain_graph=False, create_graph=False\n",
    "            )[0]\n",
    "            adv_images = adv_images.detach()\n",
    "\n",
    "            # Gradient update\n",
    "            adv_images = adv_images + alpha * grad.sign()  # Update adversarial images using the sign of the gradient\n",
    "\n",
    "            # Projection step\n",
    "            # Clamping the adversarial images to ensure they are within the Lâˆž ball of eps radius of original image\n",
    "            adv_images = torch.clamp(adv_images, images - eps, images + eps)\n",
    "\n",
    "            adv_images = adv_images.detach()\n",
    "\n",
    "        return adv_images  # Return the generated adversarial images\n",
    "\n",
    "    def pgd_batch_attack(self, eps, alpha, steps, batch_num):\n",
    "        '''\n",
    "        Launch attack for many batches and save results as class features\n",
    "        :param eps: Epsilon value in PGD attack\n",
    "        :param alpha: Alpha value in PGD attack\n",
    "        :param steps: Step value in PGD attack\n",
    "        :param batch_num: Number of batches to run the attack on\n",
    "        :return: Update attacker accuracy on original images, accuracy on adversarial images,\n",
    "        and list of adversarial images\n",
    "        '''\n",
    "        self.model.eval()\n",
    "        self.eps = eps\n",
    "        self.alpha = alpha\n",
    "        self.steps = steps\n",
    "        adv_correct = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        adv_images_lst = []\n",
    "        for i, inputs in enumerate(tqdm(self.dataloader, total=batch_num)):\n",
    "            if i == batch_num:\n",
    "                break\n",
    "            adv_images = self.pgd_attack(**inputs)\n",
    "            with torch.no_grad():\n",
    "                adv_outputs = self.model(adv_images).softmax(1)\n",
    "                adv_predictions = adv_outputs.argmax(dim=1).cpu()\n",
    "                outputs = self.model(inputs['image'].to(self.device)).softmax(1)\n",
    "                predictions = outputs.argmax(dim=1).cpu()\n",
    "            labels = inputs['label']\n",
    "            adv_correct += torch.sum(adv_predictions == labels).item()\n",
    "            correct += torch.sum(predictions == labels).item()\n",
    "            total += len(labels)\n",
    "            adv_images_lst.append(adv_images)\n",
    "        self.adv_images = torch.cat(adv_images_lst).cpu()\n",
    "        self.acc = correct / total\n",
    "        self.adv_acc = adv_correct / total\n",
    "\n",
    "    def compute_accuracy(self, batch_num):\n",
    "        '''\n",
    "        Compute model accuracy for specified number of data batches from self.dataloader\n",
    "        :param batch_num: Number of batches on which we compute model accuracy\n",
    "        :return: Update model accuracy\n",
    "        '''\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for i, inputs in enumerate(tqdm(self.dataloader, total=batch_num)):\n",
    "                if i == batch_num:\n",
    "                    break\n",
    "                inputs = {k: v.to(self.device) for (k, v) in inputs.items()}\n",
    "                outputs = self.model(inputs['image']).softmax(1)\n",
    "                predictions = outputs.argmax(dim=1)\n",
    "                correct += (predictions == inputs['label']).sum().item()\n",
    "                total += predictions.size(0)\n",
    "        self.acc = correct / total"
   ],
   "id": "29fb7354c84af91b",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 2. Functions related to generating & saving + fetching saved data from a location\n",
    "\n",
    "(TODO) : maybe this is not needed as I am creating a dataset directly!"
   ],
   "id": "4651ce982318861a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-01T08:25:21.761266Z",
     "start_time": "2024-09-01T08:25:21.746266Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def fetch_perturbed_images_from_location(location: str):\n",
    "    \"\"\"\n",
    "    Fetch and concatenate all perturbed images and labels from the specified directory.\n",
    "\n",
    "    :param location: The directory where the .pt files are saved.\n",
    "    :return: Tuple of concatenated adversarial images and labels.\n",
    "    \"\"\"\n",
    "    adv_images = []\n",
    "    labels = []\n",
    "\n",
    "    # Iterate through all .pt files in the specified directory\n",
    "    for file_name in os.listdir(location):\n",
    "        if file_name.endswith(\".pt\"):\n",
    "            file_path = os.path.join(location, file_name)\n",
    "            data = torch.load(file_path)\n",
    "            adv_images.append(data['adv_images'])\n",
    "            labels.append(data['labels'])\n",
    "\n",
    "    # Concatenate all adversarial images and labels\n",
    "    adv_images = torch.cat(adv_images)\n",
    "    labels = torch.cat(labels)\n",
    "\n",
    "    return adv_images, labels\n",
    "\n",
    "def generate_adv_images(dataloader, model, store: bool = False, store_dir: str = 'adv_images'):\n",
    "    \"\"\"\n",
    "    Generate adversarial images using the PGD method and save them in a directory.\n",
    "\n",
    "    :param dataloader: dataloader of images, labels.\n",
    "    :param model: The ResNet model used for generating adversarial images.\n",
    "    :param store: Boolean flag to indicate whether to save the generated images.\n",
    "    :param store_dir: The directory where the adversarial images and labels will be saved.\n",
    "    :return: Tuple of adversarial images and their corresponding labels.\n",
    "    \"\"\"\n",
    "    # Initialize the attacker\n",
    "    attacker = ResnetPGDAttacker(model, dataloader)\n",
    "\n",
    "    # Generate adversarial images\n",
    "    adv_images = []\n",
    "    labels = []\n",
    "    for batch in tqdm(dataloader):\n",
    "        img_batch, label_batch = batch['image'], batch['label']\n",
    "        adv_images_batch = attacker.pgd_attack(img_batch, label_batch, steps = random.randint(5, STEPS)) #randomly take 5->20 steps\n",
    "        adv_images.append(adv_images_batch)\n",
    "        labels.append(label_batch)\n",
    "\n",
    "    # Concatenate all adversarial images\n",
    "    adv_images = torch.cat(adv_images)\n",
    "    labels = torch.cat(labels)\n",
    "\n",
    "    if store:\n",
    "        # Create the store directory if it doesn't exist\n",
    "        os.makedirs(store_dir, exist_ok=True)\n",
    "    \n",
    "        # Save the adversarial images and labels in multiple files\n",
    "        for j in range(0, len(adv_images), 1000):  # Save in batches of 1000\n",
    "            start = j\n",
    "            end = min(j + 1000, len(adv_images))\n",
    "            file_path = os.path.join(store_dir, f\"adv_images_{start}-{end-1}.pt\")\n",
    "            torch.save({'adv_images': adv_images[start:end], 'labels': labels[start:end]}, file_path)\n",
    "            print(f\"Adversarial images saved at: {file_path}\")\n",
    "        \n",
    "    return store_dir, adv_images, labels"
   ],
   "id": "1e038a305a7bcbe0",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3. Creating the fine-tuning dataset",
   "id": "daebe3fda5c80640"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-01T08:25:21.780352Z",
     "start_time": "2024-09-01T08:25:21.761266Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class RobustDataLoader(DataLoader):\n",
    "    def __init__(self, dataloader, model, num_perturbations=2, num_transformations=1, device=DEVICE, **kwargs):\n",
    "        \"\"\"\n",
    "        Robust DataLoader that includes original, perturbed, and augmented images.\n",
    "\n",
    "        :param dataset: Dataset containing original images and labels.\n",
    "        :param model: The ResNet model used for generating adversarial images.\n",
    "        :param num_perturbations: Number of perturbations to generate for each original image.\n",
    "        :param num_transformations: Number of transformations to apply to each image.\n",
    "        :param kwargs: Additional arguments to be passed to the parent class constructor.\n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "        self.dataloader = dataloader\n",
    "        self.model = model.to(self.device)\n",
    "        self.num_perturbations = num_perturbations\n",
    "        self.num_transformations = num_transformations\n",
    "        self.pgd_attacker = ResnetPGDAttacker(model=self.model, dataloader=self.dataloader)\n",
    "\n",
    "        # Define transformations\n",
    "        self.transformations = [\n",
    "            transforms.Compose([\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "            ]),\n",
    "            transforms.Compose([\n",
    "                transforms.RandomVerticalFlip(),\n",
    "            ]),\n",
    "            transforms.Compose([\n",
    "                transforms.RandomRotation(20),\n",
    "            ]),\n",
    "            transforms.Compose([\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomRotation(20),\n",
    "                transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "            ]),\n",
    "            transforms.Compose([\n",
    "                transforms.RandomVerticalFlip(),\n",
    "                transforms.RandomRotation(20),\n",
    "                transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "            ])\n",
    "        ]\n",
    "\n",
    "    def perform_pgd(self, image, label):\n",
    "        random_eps = random.uniform(0.01, 0.3)\n",
    "        random_alpha = random.uniform(0.01, 0.1)\n",
    "        random_steps = 15\n",
    "        perturbed_image = self.pgd_attacker.pgd_attack(image.unsqueeze(0), label.unsqueeze(0), eps=random_eps,\n",
    "                                                  alpha=random_alpha, steps=random_steps)\n",
    "        return perturbed_image.squeeze(0).to(self.device)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for batch in self.dataloader:\n",
    "            images, labels = batch['image'], batch['label']\n",
    "            images, labels = images.to(self.device), labels.to(self.device)\n",
    "\n",
    "            original_images = []\n",
    "            original_labels = []\n",
    "            perturbed_images = []\n",
    "            transformed_images = []\n",
    "\n",
    "            for image, label in zip(images, labels):\n",
    "                # Collect original images and labels\n",
    "                original_images.append(image)\n",
    "                original_labels.append(label)\n",
    "\n",
    "                # Generate and collect perturbations\n",
    "                for _ in range(self.num_perturbations):\n",
    "                    perturbed_image = self.perform_pgd(image, label)\n",
    "                    perturbed_images.append(perturbed_image)\n",
    "                    original_labels.append(label)\n",
    "\n",
    "                # Generate and collect transformations\n",
    "                for _ in range(self.num_transformations):\n",
    "                    transformation_index = random.randint(0, len(self.transformations) - 1)\n",
    "                    transformed_image = self.transformations[transformation_index](image).to(self.device)\n",
    "                    transformed_images.append(transformed_image)\n",
    "                    original_labels.append(label)\n",
    "\n",
    "            # Concatenate images and labels into batches\n",
    "            all_images = original_images + perturbed_images + transformed_images\n",
    "            all_labels = original_labels\n",
    "\n",
    "            # Yield the batch\n",
    "            yield torch.stack(all_images), torch.tensor(all_labels)"
   ],
   "id": "1e6d83ded5e4d4f0",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-01T08:49:26.153045Z",
     "start_time": "2024-09-01T08:49:26.139541Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_dataloader(split, batch_num=1, batch_size=1):\n",
    "    BATCH_NUM = batch_num\n",
    "    BATCH_SIZE = batch_size\n",
    "    \n",
    "    # Load the dataset directly using load_dataset\n",
    "    ds = load_dataset(\"ILSVRC/imagenet-1k\", split=split, streaming=True, trust_remote_code=True)\n",
    "    \n",
    "    # Preprocess the dataset\n",
    "    ds = ds.filter(lambda example: example['image'].mode == 'RGB')\n",
    "    ds = ds.map(preprocess_img)\n",
    "    ds = ds.shuffle(seed=SEED)\n",
    "    ds = ds.take(BATCH_NUM * BATCH_SIZE)\n",
    "    \n",
    "    # Create a DataLoader for the original dataset\n",
    "    dataloader = DataLoader(ds, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    # Create the RobustDataLoader\n",
    "    return RobustDataLoader(dataloader, model, num_perturbations=1, num_transformations=1, batch_size=BATCH_SIZE, shuffle=True)\n"
   ],
   "id": "2529e44c8e523535",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-01T08:50:55.278024Z",
     "start_time": "2024-09-01T08:49:26.540632Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_loader = create_dataloader(\"train\")\n",
    "val_loader = create_dataloader(\"val\")\n",
    "test_loader = create_dataloader(\"test\")"
   ],
   "id": "5095391b088ee33e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Downloading data:   0%|          | 0.00/29.1G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f0f824c6a18f4026bbe02708b9307cd4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[18], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m train_loader \u001B[38;5;241m=\u001B[39m \u001B[43mcreate_dataloader\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtrain\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      2\u001B[0m val_loader \u001B[38;5;241m=\u001B[39m create_dataloader(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mval\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      3\u001B[0m test_loader \u001B[38;5;241m=\u001B[39m create_dataloader(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[1;32mIn[17], line 6\u001B[0m, in \u001B[0;36mcreate_dataloader\u001B[1;34m(split, batch_num, batch_size)\u001B[0m\n\u001B[0;32m      3\u001B[0m BATCH_SIZE \u001B[38;5;241m=\u001B[39m batch_size\n\u001B[0;32m      5\u001B[0m \u001B[38;5;66;03m# Load the dataset directly using load_dataset\u001B[39;00m\n\u001B[1;32m----> 6\u001B[0m ds \u001B[38;5;241m=\u001B[39m \u001B[43mload_dataset\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mILSVRC/imagenet-1k\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msplit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msplit\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstreaming\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrust_remote_code\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;66;03m# Preprocess the dataset\u001B[39;00m\n\u001B[0;32m      9\u001B[0m ds \u001B[38;5;241m=\u001B[39m ds\u001B[38;5;241m.\u001B[39mfilter(\u001B[38;5;28;01mlambda\u001B[39;00m example: example[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mimage\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mmode \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mRGB\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\datasets\\load.py:2628\u001B[0m, in \u001B[0;36mload_dataset\u001B[1;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001B[0m\n\u001B[0;32m   2625\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m builder_instance\u001B[38;5;241m.\u001B[39mas_streaming_dataset(split\u001B[38;5;241m=\u001B[39msplit)\n\u001B[0;32m   2627\u001B[0m \u001B[38;5;66;03m# Download and prepare data\u001B[39;00m\n\u001B[1;32m-> 2628\u001B[0m \u001B[43mbuilder_instance\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdownload_and_prepare\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   2629\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdownload_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2630\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdownload_mode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_mode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2631\u001B[0m \u001B[43m    \u001B[49m\u001B[43mverification_mode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverification_mode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2632\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnum_proc\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_proc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2633\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstorage_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2634\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2636\u001B[0m \u001B[38;5;66;03m# Build dataset for splits\u001B[39;00m\n\u001B[0;32m   2637\u001B[0m keep_in_memory \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m   2638\u001B[0m     keep_in_memory \u001B[38;5;28;01mif\u001B[39;00m keep_in_memory \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m is_small_dataset(builder_instance\u001B[38;5;241m.\u001B[39minfo\u001B[38;5;241m.\u001B[39mdataset_size)\n\u001B[0;32m   2639\u001B[0m )\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\datasets\\builder.py:1029\u001B[0m, in \u001B[0;36mDatasetBuilder.download_and_prepare\u001B[1;34m(self, output_dir, download_config, download_mode, verification_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001B[0m\n\u001B[0;32m   1027\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m num_proc \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   1028\u001B[0m         prepare_split_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnum_proc\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m num_proc\n\u001B[1;32m-> 1029\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_download_and_prepare(\n\u001B[0;32m   1030\u001B[0m         dl_manager\u001B[38;5;241m=\u001B[39mdl_manager,\n\u001B[0;32m   1031\u001B[0m         verification_mode\u001B[38;5;241m=\u001B[39mverification_mode,\n\u001B[0;32m   1032\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mprepare_split_kwargs,\n\u001B[0;32m   1033\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mdownload_and_prepare_kwargs,\n\u001B[0;32m   1034\u001B[0m     )\n\u001B[0;32m   1035\u001B[0m \u001B[38;5;66;03m# Sync info\u001B[39;00m\n\u001B[0;32m   1036\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minfo\u001B[38;5;241m.\u001B[39mdataset_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msum\u001B[39m(split\u001B[38;5;241m.\u001B[39mnum_bytes \u001B[38;5;28;01mfor\u001B[39;00m split \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minfo\u001B[38;5;241m.\u001B[39msplits\u001B[38;5;241m.\u001B[39mvalues())\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\datasets\\builder.py:1791\u001B[0m, in \u001B[0;36mGeneratorBasedBuilder._download_and_prepare\u001B[1;34m(self, dl_manager, verification_mode, **prepare_splits_kwargs)\u001B[0m\n\u001B[0;32m   1790\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_download_and_prepare\u001B[39m(\u001B[38;5;28mself\u001B[39m, dl_manager, verification_mode, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mprepare_splits_kwargs):\n\u001B[1;32m-> 1791\u001B[0m     \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m_download_and_prepare(\n\u001B[0;32m   1792\u001B[0m         dl_manager,\n\u001B[0;32m   1793\u001B[0m         verification_mode,\n\u001B[0;32m   1794\u001B[0m         check_duplicate_keys\u001B[38;5;241m=\u001B[39mverification_mode \u001B[38;5;241m==\u001B[39m VerificationMode\u001B[38;5;241m.\u001B[39mBASIC_CHECKS\n\u001B[0;32m   1795\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m verification_mode \u001B[38;5;241m==\u001B[39m VerificationMode\u001B[38;5;241m.\u001B[39mALL_CHECKS,\n\u001B[0;32m   1796\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mprepare_splits_kwargs,\n\u001B[0;32m   1797\u001B[0m     )\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\datasets\\builder.py:1102\u001B[0m, in \u001B[0;36mDatasetBuilder._download_and_prepare\u001B[1;34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001B[0m\n\u001B[0;32m   1100\u001B[0m split_dict \u001B[38;5;241m=\u001B[39m SplitDict(dataset_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset_name)\n\u001B[0;32m   1101\u001B[0m split_generators_kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_split_generators_kwargs(prepare_split_kwargs)\n\u001B[1;32m-> 1102\u001B[0m split_generators \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_split_generators(dl_manager, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39msplit_generators_kwargs)\n\u001B[0;32m   1104\u001B[0m \u001B[38;5;66;03m# Checksums verification\u001B[39;00m\n\u001B[0;32m   1105\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m verification_mode \u001B[38;5;241m==\u001B[39m VerificationMode\u001B[38;5;241m.\u001B[39mALL_CHECKS \u001B[38;5;129;01mand\u001B[39;00m dl_manager\u001B[38;5;241m.\u001B[39mrecord_checksums:\n",
      "File \u001B[1;32m~\\.cache\\huggingface\\modules\\datasets_modules\\datasets\\ILSVRC--imagenet-1k\\07900defe1ccf3404ea7e5e876a64ca41192f6c07406044771544ef1505831e8\\imagenet-1k.py:70\u001B[0m, in \u001B[0;36mImagenet1k._split_generators\u001B[1;34m(self, dl_manager)\u001B[0m\n\u001B[0;32m     68\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_split_generators\u001B[39m(\u001B[38;5;28mself\u001B[39m, dl_manager):\n\u001B[0;32m     69\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Returns SplitGenerators.\"\"\"\u001B[39;00m\n\u001B[1;32m---> 70\u001B[0m     archives \u001B[38;5;241m=\u001B[39m \u001B[43mdl_manager\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdownload\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_DATA_URL\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     72\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [\n\u001B[0;32m     73\u001B[0m         datasets\u001B[38;5;241m.\u001B[39mSplitGenerator(\n\u001B[0;32m     74\u001B[0m             name\u001B[38;5;241m=\u001B[39mdatasets\u001B[38;5;241m.\u001B[39mSplit\u001B[38;5;241m.\u001B[39mTRAIN,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     93\u001B[0m         ),\n\u001B[0;32m     94\u001B[0m     ]\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\datasets\\download\\download_manager.py:257\u001B[0m, in \u001B[0;36mDownloadManager.download\u001B[1;34m(self, url_or_urls)\u001B[0m\n\u001B[0;32m    255\u001B[0m start_time \u001B[38;5;241m=\u001B[39m datetime\u001B[38;5;241m.\u001B[39mnow()\n\u001B[0;32m    256\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m stack_multiprocessing_download_progress_bars():\n\u001B[1;32m--> 257\u001B[0m     downloaded_path_or_paths \u001B[38;5;241m=\u001B[39m \u001B[43mmap_nested\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    258\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdownload_func\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    259\u001B[0m \u001B[43m        \u001B[49m\u001B[43murl_or_urls\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    260\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmap_tuple\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    261\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnum_proc\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_config\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnum_proc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    262\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdesc\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mDownloading data files\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    263\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbatched\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    264\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    265\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    266\u001B[0m duration \u001B[38;5;241m=\u001B[39m datetime\u001B[38;5;241m.\u001B[39mnow() \u001B[38;5;241m-\u001B[39m start_time\n\u001B[0;32m    267\u001B[0m logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDownloading took \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mduration\u001B[38;5;241m.\u001B[39mtotal_seconds()\u001B[38;5;250m \u001B[39m\u001B[38;5;241m/\u001B[39m\u001B[38;5;241m/\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m60\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m min\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\datasets\\utils\\py_utils.py:494\u001B[0m, in \u001B[0;36mmap_nested\u001B[1;34m(function, data_struct, dict_only, map_list, map_tuple, map_numpy, num_proc, parallel_min_length, batched, batch_size, types, disable_tqdm, desc)\u001B[0m\n\u001B[0;32m    492\u001B[0m     num_proc \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    493\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28many\u001B[39m(\u001B[38;5;28misinstance\u001B[39m(v, types) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(v) \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mlen\u001B[39m(iterable) \u001B[38;5;28;01mfor\u001B[39;00m v \u001B[38;5;129;01min\u001B[39;00m iterable):\n\u001B[1;32m--> 494\u001B[0m     mapped \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m    495\u001B[0m         map_nested(\n\u001B[0;32m    496\u001B[0m             function\u001B[38;5;241m=\u001B[39mfunction,\n\u001B[0;32m    497\u001B[0m             data_struct\u001B[38;5;241m=\u001B[39mobj,\n\u001B[0;32m    498\u001B[0m             num_proc\u001B[38;5;241m=\u001B[39mnum_proc,\n\u001B[0;32m    499\u001B[0m             parallel_min_length\u001B[38;5;241m=\u001B[39mparallel_min_length,\n\u001B[0;32m    500\u001B[0m             batched\u001B[38;5;241m=\u001B[39mbatched,\n\u001B[0;32m    501\u001B[0m             batch_size\u001B[38;5;241m=\u001B[39mbatch_size,\n\u001B[0;32m    502\u001B[0m             types\u001B[38;5;241m=\u001B[39mtypes,\n\u001B[0;32m    503\u001B[0m         )\n\u001B[0;32m    504\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m obj \u001B[38;5;129;01min\u001B[39;00m iterable\n\u001B[0;32m    505\u001B[0m     ]\n\u001B[0;32m    506\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m num_proc \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m num_proc \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(iterable) \u001B[38;5;241m<\u001B[39m parallel_min_length:\n\u001B[0;32m    507\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m batched:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\datasets\\utils\\py_utils.py:495\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    492\u001B[0m     num_proc \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    493\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28many\u001B[39m(\u001B[38;5;28misinstance\u001B[39m(v, types) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(v) \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mlen\u001B[39m(iterable) \u001B[38;5;28;01mfor\u001B[39;00m v \u001B[38;5;129;01min\u001B[39;00m iterable):\n\u001B[0;32m    494\u001B[0m     mapped \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m--> 495\u001B[0m         \u001B[43mmap_nested\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    496\u001B[0m \u001B[43m            \u001B[49m\u001B[43mfunction\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfunction\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    497\u001B[0m \u001B[43m            \u001B[49m\u001B[43mdata_struct\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mobj\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    498\u001B[0m \u001B[43m            \u001B[49m\u001B[43mnum_proc\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_proc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    499\u001B[0m \u001B[43m            \u001B[49m\u001B[43mparallel_min_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparallel_min_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    500\u001B[0m \u001B[43m            \u001B[49m\u001B[43mbatched\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatched\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    501\u001B[0m \u001B[43m            \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    502\u001B[0m \u001B[43m            \u001B[49m\u001B[43mtypes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtypes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    503\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    504\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m obj \u001B[38;5;129;01min\u001B[39;00m iterable\n\u001B[0;32m    505\u001B[0m     ]\n\u001B[0;32m    506\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m num_proc \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m num_proc \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(iterable) \u001B[38;5;241m<\u001B[39m parallel_min_length:\n\u001B[0;32m    507\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m batched:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\datasets\\utils\\py_utils.py:511\u001B[0m, in \u001B[0;36mmap_nested\u001B[1;34m(function, data_struct, dict_only, map_list, map_tuple, map_numpy, num_proc, parallel_min_length, batched, batch_size, types, disable_tqdm, desc)\u001B[0m\n\u001B[0;32m    509\u001B[0m         batch_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmax\u001B[39m(\u001B[38;5;28mlen\u001B[39m(iterable) \u001B[38;5;241m/\u001B[39m\u001B[38;5;241m/\u001B[39m num_proc \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mint\u001B[39m(\u001B[38;5;28mlen\u001B[39m(iterable) \u001B[38;5;241m%\u001B[39m num_proc \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m), \u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m    510\u001B[0m     iterable \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(iter_batched(iterable, batch_size))\n\u001B[1;32m--> 511\u001B[0m mapped \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m    512\u001B[0m     _single_map_nested((function, obj, batched, batch_size, types, \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;28;01mTrue\u001B[39;00m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[0;32m    513\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m obj \u001B[38;5;129;01min\u001B[39;00m hf_tqdm(iterable, disable\u001B[38;5;241m=\u001B[39mdisable_tqdm, desc\u001B[38;5;241m=\u001B[39mdesc)\n\u001B[0;32m    514\u001B[0m ]\n\u001B[0;32m    515\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m batched:\n\u001B[0;32m    516\u001B[0m     mapped \u001B[38;5;241m=\u001B[39m [mapped_item \u001B[38;5;28;01mfor\u001B[39;00m mapped_batch \u001B[38;5;129;01min\u001B[39;00m mapped \u001B[38;5;28;01mfor\u001B[39;00m mapped_item \u001B[38;5;129;01min\u001B[39;00m mapped_batch]\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\datasets\\utils\\py_utils.py:512\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    509\u001B[0m         batch_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmax\u001B[39m(\u001B[38;5;28mlen\u001B[39m(iterable) \u001B[38;5;241m/\u001B[39m\u001B[38;5;241m/\u001B[39m num_proc \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mint\u001B[39m(\u001B[38;5;28mlen\u001B[39m(iterable) \u001B[38;5;241m%\u001B[39m num_proc \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m), \u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m    510\u001B[0m     iterable \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(iter_batched(iterable, batch_size))\n\u001B[0;32m    511\u001B[0m mapped \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m--> 512\u001B[0m     \u001B[43m_single_map_nested\u001B[49m\u001B[43m(\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfunction\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mobj\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatched\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtypes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    513\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m obj \u001B[38;5;129;01min\u001B[39;00m hf_tqdm(iterable, disable\u001B[38;5;241m=\u001B[39mdisable_tqdm, desc\u001B[38;5;241m=\u001B[39mdesc)\n\u001B[0;32m    514\u001B[0m ]\n\u001B[0;32m    515\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m batched:\n\u001B[0;32m    516\u001B[0m     mapped \u001B[38;5;241m=\u001B[39m [mapped_item \u001B[38;5;28;01mfor\u001B[39;00m mapped_batch \u001B[38;5;129;01min\u001B[39;00m mapped \u001B[38;5;28;01mfor\u001B[39;00m mapped_item \u001B[38;5;129;01min\u001B[39;00m mapped_batch]\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\datasets\\utils\\py_utils.py:380\u001B[0m, in \u001B[0;36m_single_map_nested\u001B[1;34m(args)\u001B[0m\n\u001B[0;32m    373\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m function(data_struct)\n\u001B[0;32m    374\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m    375\u001B[0m     batched\n\u001B[0;32m    376\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data_struct, \u001B[38;5;28mdict\u001B[39m)\n\u001B[0;32m    377\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data_struct, types)\n\u001B[0;32m    378\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mall\u001B[39m(\u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(v, (\u001B[38;5;28mdict\u001B[39m, types)) \u001B[38;5;28;01mfor\u001B[39;00m v \u001B[38;5;129;01min\u001B[39;00m data_struct)\n\u001B[0;32m    379\u001B[0m ):\n\u001B[1;32m--> 380\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [mapped_item \u001B[38;5;28;01mfor\u001B[39;00m batch \u001B[38;5;129;01min\u001B[39;00m iter_batched(data_struct, batch_size) \u001B[38;5;28;01mfor\u001B[39;00m mapped_item \u001B[38;5;129;01min\u001B[39;00m function(batch)]\n\u001B[0;32m    382\u001B[0m \u001B[38;5;66;03m# Reduce logging to keep things readable in multiprocessing with tqdm\u001B[39;00m\n\u001B[0;32m    383\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m rank \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m logging\u001B[38;5;241m.\u001B[39mget_verbosity() \u001B[38;5;241m<\u001B[39m logging\u001B[38;5;241m.\u001B[39mWARNING:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\datasets\\utils\\py_utils.py:380\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    373\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m function(data_struct)\n\u001B[0;32m    374\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m    375\u001B[0m     batched\n\u001B[0;32m    376\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data_struct, \u001B[38;5;28mdict\u001B[39m)\n\u001B[0;32m    377\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data_struct, types)\n\u001B[0;32m    378\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mall\u001B[39m(\u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(v, (\u001B[38;5;28mdict\u001B[39m, types)) \u001B[38;5;28;01mfor\u001B[39;00m v \u001B[38;5;129;01min\u001B[39;00m data_struct)\n\u001B[0;32m    379\u001B[0m ):\n\u001B[1;32m--> 380\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [mapped_item \u001B[38;5;28;01mfor\u001B[39;00m batch \u001B[38;5;129;01min\u001B[39;00m iter_batched(data_struct, batch_size) \u001B[38;5;28;01mfor\u001B[39;00m mapped_item \u001B[38;5;129;01min\u001B[39;00m \u001B[43mfunction\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m)\u001B[49m]\n\u001B[0;32m    382\u001B[0m \u001B[38;5;66;03m# Reduce logging to keep things readable in multiprocessing with tqdm\u001B[39;00m\n\u001B[0;32m    383\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m rank \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m logging\u001B[38;5;241m.\u001B[39mget_verbosity() \u001B[38;5;241m<\u001B[39m logging\u001B[38;5;241m.\u001B[39mWARNING:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\datasets\\download\\download_manager.py:313\u001B[0m, in \u001B[0;36mDownloadManager._download_batched\u001B[1;34m(self, url_or_filenames, download_config)\u001B[0m\n\u001B[0;32m    300\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m thread_map(\n\u001B[0;32m    301\u001B[0m         download_func,\n\u001B[0;32m    302\u001B[0m         url_or_filenames,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    310\u001B[0m         tqdm_class\u001B[38;5;241m=\u001B[39mtqdm,\n\u001B[0;32m    311\u001B[0m     )\n\u001B[0;32m    312\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 313\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [\n\u001B[0;32m    314\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_download_single(url_or_filename, download_config\u001B[38;5;241m=\u001B[39mdownload_config)\n\u001B[0;32m    315\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m url_or_filename \u001B[38;5;129;01min\u001B[39;00m url_or_filenames\n\u001B[0;32m    316\u001B[0m     ]\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\datasets\\download\\download_manager.py:314\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    300\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m thread_map(\n\u001B[0;32m    301\u001B[0m         download_func,\n\u001B[0;32m    302\u001B[0m         url_or_filenames,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    310\u001B[0m         tqdm_class\u001B[38;5;241m=\u001B[39mtqdm,\n\u001B[0;32m    311\u001B[0m     )\n\u001B[0;32m    312\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    313\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [\n\u001B[1;32m--> 314\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_download_single\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl_or_filename\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdownload_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_config\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    315\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m url_or_filename \u001B[38;5;129;01min\u001B[39;00m url_or_filenames\n\u001B[0;32m    316\u001B[0m     ]\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\datasets\\download\\download_manager.py:323\u001B[0m, in \u001B[0;36mDownloadManager._download_single\u001B[1;34m(self, url_or_filename, download_config)\u001B[0m\n\u001B[0;32m    320\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_relative_path(url_or_filename):\n\u001B[0;32m    321\u001B[0m     \u001B[38;5;66;03m# append the relative path to the base_path\u001B[39;00m\n\u001B[0;32m    322\u001B[0m     url_or_filename \u001B[38;5;241m=\u001B[39m url_or_path_join(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_base_path, url_or_filename)\n\u001B[1;32m--> 323\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[43mcached_path\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl_or_filename\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdownload_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_config\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    324\u001B[0m out \u001B[38;5;241m=\u001B[39m tracked_str(out)\n\u001B[0;32m    325\u001B[0m out\u001B[38;5;241m.\u001B[39mset_origin(url_or_filename)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\datasets\\utils\\file_utils.py:211\u001B[0m, in \u001B[0;36mcached_path\u001B[1;34m(url_or_filename, download_config, **download_kwargs)\u001B[0m\n\u001B[0;32m    205\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m    206\u001B[0m         storage_options\n\u001B[0;32m    207\u001B[0m         \u001B[38;5;129;01mand\u001B[39;00m storage_options\u001B[38;5;241m.\u001B[39mkeys() \u001B[38;5;241m<\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttp\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps\u001B[39m\u001B[38;5;124m\"\u001B[39m}\n\u001B[0;32m    208\u001B[0m         \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (download_config\u001B[38;5;241m.\u001B[39mstorage_options \u001B[38;5;129;01mand\u001B[39;00m download_config\u001B[38;5;241m.\u001B[39mstorage_options\u001B[38;5;241m.\u001B[39mkeys() \u001B[38;5;241m<\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttp\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps\u001B[39m\u001B[38;5;124m\"\u001B[39m})\n\u001B[0;32m    209\u001B[0m     ):\n\u001B[0;32m    210\u001B[0m         storage_options \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m--> 211\u001B[0m     output_path \u001B[38;5;241m=\u001B[39m \u001B[43mget_from_cache\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    212\u001B[0m \u001B[43m        \u001B[49m\u001B[43murl_or_filename\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    213\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    214\u001B[0m \u001B[43m        \u001B[49m\u001B[43mforce_download\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_config\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforce_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    215\u001B[0m \u001B[43m        \u001B[49m\u001B[43mproxies\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_config\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mproxies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    216\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresume_download\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_config\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresume_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    217\u001B[0m \u001B[43m        \u001B[49m\u001B[43muser_agent\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_config\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43muser_agent\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    218\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_config\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    219\u001B[0m \u001B[43m        \u001B[49m\u001B[43muse_etag\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_config\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43muse_etag\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    220\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_retries\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_config\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_retries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    221\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtoken\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_config\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtoken\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    222\u001B[0m \u001B[43m        \u001B[49m\u001B[43mignore_url_params\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_config\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mignore_url_params\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    223\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstorage_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    224\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdownload_desc\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_config\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdownload_desc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    225\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdisable_tqdm\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_config\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdisable_tqdm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    226\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    227\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mexists(url_or_filename):\n\u001B[0;32m    228\u001B[0m     \u001B[38;5;66;03m# File, and it exists.\u001B[39;00m\n\u001B[0;32m    229\u001B[0m     output_path \u001B[38;5;241m=\u001B[39m url_or_filename\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\datasets\\utils\\file_utils.py:689\u001B[0m, in \u001B[0;36mget_from_cache\u001B[1;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, local_files_only, use_etag, max_retries, token, use_auth_token, ignore_url_params, storage_options, download_desc, disable_tqdm)\u001B[0m\n\u001B[0;32m    687\u001B[0m     ftp_get(url, temp_file)\n\u001B[0;32m    688\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m scheme \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttp\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps\u001B[39m\u001B[38;5;124m\"\u001B[39m} \u001B[38;5;129;01mor\u001B[39;00m storage_options\u001B[38;5;241m.\u001B[39mget(scheme):\n\u001B[1;32m--> 689\u001B[0m     \u001B[43mfsspec_get\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    690\u001B[0m \u001B[43m        \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtemp_file\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstorage_options\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdesc\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_desc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdisable_tqdm\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdisable_tqdm\u001B[49m\n\u001B[0;32m    691\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    692\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    693\u001B[0m     http_get(\n\u001B[0;32m    694\u001B[0m         url,\n\u001B[0;32m    695\u001B[0m         temp_file\u001B[38;5;241m=\u001B[39mtemp_file,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    702\u001B[0m         disable_tqdm\u001B[38;5;241m=\u001B[39mdisable_tqdm,\n\u001B[0;32m    703\u001B[0m     )\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\datasets\\utils\\file_utils.py:395\u001B[0m, in \u001B[0;36mfsspec_get\u001B[1;34m(url, temp_file, storage_options, desc, disable_tqdm)\u001B[0m\n\u001B[0;32m    382\u001B[0m fs, path \u001B[38;5;241m=\u001B[39m url_to_fs(url, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m(storage_options \u001B[38;5;129;01mor\u001B[39;00m {}))\n\u001B[0;32m    383\u001B[0m callback \u001B[38;5;241m=\u001B[39m TqdmCallback(\n\u001B[0;32m    384\u001B[0m     tqdm_kwargs\u001B[38;5;241m=\u001B[39m{\n\u001B[0;32m    385\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdesc\u001B[39m\u001B[38;5;124m\"\u001B[39m: desc \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDownloading\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    393\u001B[0m     }\n\u001B[0;32m    394\u001B[0m )\n\u001B[1;32m--> 395\u001B[0m \u001B[43mfs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_file\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtemp_file\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallback\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallback\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\huggingface_hub\\hf_file_system.py:648\u001B[0m, in \u001B[0;36mHfFileSystem.get_file\u001B[1;34m(self, rpath, lpath, callback, outfile, **kwargs)\u001B[0m\n\u001B[0;32m    646\u001B[0m callback\u001B[38;5;241m.\u001B[39mset_size(expected_size)\n\u001B[0;32m    647\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 648\u001B[0m     \u001B[43mhttp_get\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    649\u001B[0m \u001B[43m        \u001B[49m\u001B[43murl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhf_hub_url\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    650\u001B[0m \u001B[43m            \u001B[49m\u001B[43mrepo_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresolve_remote_path\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrepo_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    651\u001B[0m \u001B[43m            \u001B[49m\u001B[43mrevision\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresolve_remote_path\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrevision\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    652\u001B[0m \u001B[43m            \u001B[49m\u001B[43mfilename\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresolve_remote_path\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpath_in_repo\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    653\u001B[0m \u001B[43m            \u001B[49m\u001B[43mrepo_type\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresolve_remote_path\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrepo_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    654\u001B[0m \u001B[43m            \u001B[49m\u001B[43mendpoint\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mendpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    655\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    656\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtemp_file\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutfile\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    657\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdisplayed_filename\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrpath\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    658\u001B[0m \u001B[43m        \u001B[49m\u001B[43mexpected_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mexpected_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    659\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresume_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    660\u001B[0m \u001B[43m        \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_api\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_build_hf_headers\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    661\u001B[0m \u001B[43m        \u001B[49m\u001B[43m_tqdm_bar\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallback\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtqdm\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43misinstance\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcallback\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mTqdmCallback\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    662\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    663\u001B[0m     outfile\u001B[38;5;241m.\u001B[39mseek(initial_pos)\n\u001B[0;32m    664\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    665\u001B[0m     \u001B[38;5;66;03m# Close file only if we opened it ourselves\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\huggingface_hub\\file_download.py:549\u001B[0m, in \u001B[0;36mhttp_get\u001B[1;34m(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries, _tqdm_bar)\u001B[0m\n\u001B[0;32m    547\u001B[0m new_resume_size \u001B[38;5;241m=\u001B[39m resume_size\n\u001B[0;32m    548\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 549\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m chunk \u001B[38;5;129;01min\u001B[39;00m r\u001B[38;5;241m.\u001B[39miter_content(chunk_size\u001B[38;5;241m=\u001B[39mDOWNLOAD_CHUNK_SIZE):\n\u001B[0;32m    550\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m chunk:  \u001B[38;5;66;03m# filter out keep-alive new chunks\u001B[39;00m\n\u001B[0;32m    551\u001B[0m             progress\u001B[38;5;241m.\u001B[39mupdate(\u001B[38;5;28mlen\u001B[39m(chunk))\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\requests\\models.py:820\u001B[0m, in \u001B[0;36mResponse.iter_content.<locals>.generate\u001B[1;34m()\u001B[0m\n\u001B[0;32m    818\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mraw, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstream\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m    819\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 820\u001B[0m         \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mraw\u001B[38;5;241m.\u001B[39mstream(chunk_size, decode_content\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m    821\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m ProtocolError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    822\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m ChunkedEncodingError(e)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\urllib3\\response.py:1060\u001B[0m, in \u001B[0;36mHTTPResponse.stream\u001B[1;34m(self, amt, decode_content)\u001B[0m\n\u001B[0;32m   1058\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1059\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_fp_closed(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fp) \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_decoded_buffer) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m-> 1060\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mamt\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mamt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdecode_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdecode_content\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1062\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m data:\n\u001B[0;32m   1063\u001B[0m             \u001B[38;5;28;01myield\u001B[39;00m data\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\urllib3\\response.py:949\u001B[0m, in \u001B[0;36mHTTPResponse.read\u001B[1;34m(self, amt, decode_content, cache_content)\u001B[0m\n\u001B[0;32m    946\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_decoded_buffer) \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m amt:\n\u001B[0;32m    947\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_decoded_buffer\u001B[38;5;241m.\u001B[39mget(amt)\n\u001B[1;32m--> 949\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_raw_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mamt\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    951\u001B[0m flush_decoder \u001B[38;5;241m=\u001B[39m amt \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m (amt \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m data)\n\u001B[0;32m    953\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m data \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_decoded_buffer) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\urllib3\\response.py:873\u001B[0m, in \u001B[0;36mHTTPResponse._raw_read\u001B[1;34m(self, amt, read1)\u001B[0m\n\u001B[0;32m    870\u001B[0m fp_closed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fp, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclosed\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m    872\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_error_catcher():\n\u001B[1;32m--> 873\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fp_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mamt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mread1\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mread1\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m fp_closed \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    874\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m amt \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m amt \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m data:\n\u001B[0;32m    875\u001B[0m         \u001B[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001B[39;00m\n\u001B[0;32m    876\u001B[0m         \u001B[38;5;66;03m# Close the connection when no data is returned\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    881\u001B[0m         \u001B[38;5;66;03m# not properly close the connection in all cases. There is\u001B[39;00m\n\u001B[0;32m    882\u001B[0m         \u001B[38;5;66;03m# no harm in redundantly calling close.\u001B[39;00m\n\u001B[0;32m    883\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fp\u001B[38;5;241m.\u001B[39mclose()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\urllib3\\response.py:856\u001B[0m, in \u001B[0;36mHTTPResponse._fp_read\u001B[1;34m(self, amt, read1)\u001B[0m\n\u001B[0;32m    853\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fp\u001B[38;5;241m.\u001B[39mread1(amt) \u001B[38;5;28;01mif\u001B[39;00m amt \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fp\u001B[38;5;241m.\u001B[39mread1()\n\u001B[0;32m    854\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    855\u001B[0m     \u001B[38;5;66;03m# StringIO doesn't like amt=None\u001B[39;00m\n\u001B[1;32m--> 856\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mamt\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mif\u001B[39;00m amt \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fp\u001B[38;5;241m.\u001B[39mread()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\http\\client.py:465\u001B[0m, in \u001B[0;36mHTTPResponse.read\u001B[1;34m(self, amt)\u001B[0m\n\u001B[0;32m    462\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlength \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m amt \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlength:\n\u001B[0;32m    463\u001B[0m     \u001B[38;5;66;03m# clip the read to the \"end of response\"\u001B[39;00m\n\u001B[0;32m    464\u001B[0m     amt \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlength\n\u001B[1;32m--> 465\u001B[0m s \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mamt\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    466\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m s \u001B[38;5;129;01mand\u001B[39;00m amt:\n\u001B[0;32m    467\u001B[0m     \u001B[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001B[39;00m\n\u001B[0;32m    468\u001B[0m     \u001B[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001B[39;00m\n\u001B[0;32m    469\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_close_conn()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\socket.py:705\u001B[0m, in \u001B[0;36mSocketIO.readinto\u001B[1;34m(self, b)\u001B[0m\n\u001B[0;32m    703\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m    704\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 705\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sock\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrecv_into\u001B[49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    706\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m timeout:\n\u001B[0;32m    707\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_timeout_occurred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\ssl.py:1274\u001B[0m, in \u001B[0;36mSSLSocket.recv_into\u001B[1;34m(self, buffer, nbytes, flags)\u001B[0m\n\u001B[0;32m   1270\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m flags \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m   1271\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   1272\u001B[0m           \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m\n\u001B[0;32m   1273\u001B[0m           \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m)\n\u001B[1;32m-> 1274\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnbytes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbuffer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1275\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1276\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\ssl.py:1130\u001B[0m, in \u001B[0;36mSSLSocket.read\u001B[1;34m(self, len, buffer)\u001B[0m\n\u001B[0;32m   1128\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1129\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m buffer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m-> 1130\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sslobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbuffer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1131\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1132\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sslobj\u001B[38;5;241m.\u001B[39mread(\u001B[38;5;28mlen\u001B[39m)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 4. Implementing Robust Resnet",
   "id": "f2b608c00b6103c6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-01T08:59:35.834046Z",
     "start_time": "2024-09-01T08:59:35.783473Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class RobustResnet(LightningModule):\n",
    "    def __init__(self, model=model, train_loader=train_loader, val_loader=val_loader, test_loader=test_loader,\n",
    "                 learning_rate=1e-3):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.train_dataloader = train_loader\n",
    "        self.val_dataloader = val_loader\n",
    "        self.test_dataloader = test_loader\n",
    "        self.learning_rate = learning_rate\n",
    "        self.train_acc = Accuracy()\n",
    "        self.val_acc = Accuracy()\n",
    "        self.test_acc = Accuracy()\n",
    "\n",
    "        # Initialize the PGD attacker once\n",
    "        self.pgd_attacker = ResnetPGDAttacker(model=self.model, dataloader=self.train_dataloader)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "        logits = self.model(images)\n",
    "        ce_loss = F.cross_entropy(logits, labels)\n",
    "        pgd_loss = self.pgd_loss(images, labels)\n",
    "        loss = ce_loss + pgd_loss\n",
    "\n",
    "        self.train_acc(logits, labels)\n",
    "        self.log('train_ce_loss', ce_loss)\n",
    "        self.log('train_pgd_loss', pgd_loss)\n",
    "        self.log('train_loss', loss)\n",
    "        self.log('train_acc', self.train_acc, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "        logits = self.model(images)\n",
    "        ce_loss = F.cross_entropy(logits, labels)\n",
    "        pgd_loss = self.pgd_loss(images, labels)\n",
    "        loss = ce_loss + pgd_loss\n",
    "        self.val_acc(logits, labels)\n",
    "        self.log('val_ce_loss', ce_loss, on_step=False, on_epoch=True)\n",
    "        self.log('val_pgd_loss', pgd_loss, on_step=False, on_epoch=True)\n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True)\n",
    "        self.log('val_acc', self.val_acc, on_step=False, on_epoch=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "        logits = self.model(images)\n",
    "        ce_loss = F.cross_entropy(logits, labels)\n",
    "        pgd_loss = self.pgd_loss(images, labels)\n",
    "        loss = ce_loss + pgd_loss\n",
    "        acc = self.test_acc(logits, labels)\n",
    "        self.log('test_ce_loss', ce_loss)\n",
    "        self.log('test_pgd_loss', pgd_loss)\n",
    "        self.log('test_loss', loss)\n",
    "        self.log('test_acc', acc)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    def pgd_loss(self, images, labels):\n",
    "        # Generate adversarial examples and calculate the loss\n",
    "        adv_images = self.pgd_attacker.pgd_attack(images.to(self.device), labels.to(self.device))\n",
    "        adv_logits = self.model(adv_images)\n",
    "        pgd_loss = F.cross_entropy(adv_logits, labels.to(self.device))\n",
    "\n",
    "        return pgd_loss\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self.train_dataloader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.val_dataloader\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return self.test_dataloader"
   ],
   "id": "5b8ccd73ff3dcac6",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LightningModule' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[19], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mRobustResnet\u001B[39;00m(\u001B[43mLightningModule\u001B[49m):\n\u001B[0;32m      2\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, model, train_dataloader, val_dataloader, test_dataloader, learning_rate\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1e-3\u001B[39m):\n\u001B[0;32m      3\u001B[0m         \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m()\n",
      "\u001B[1;31mNameError\u001B[0m: name 'LightningModule' is not defined"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create the classifier\n",
    "classifier = RobustResnet(model, train_loader, val_loader, test_loader)\n",
    "\n",
    "# Set up the checkpoint callback\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',  # Monitor validation loss for saving the best model\n",
    "    dirpath='checkpoints/',  # Directory where checkpoints will be saved\n",
    "    filename='best-checkpoint',  # Checkpoint filename\n",
    "    save_top_k=1,  # Save only the best model\n",
    "    mode='min'  # Minimize the monitored metric\n",
    ")\n",
    "\n",
    "# Create a Trainer\n",
    "trainer = Trainer(\n",
    "    max_epochs=10,\n",
    "    gpus=1,  # Use 1 GPU if available\n",
    "    callbacks=[checkpoint_callback],  # Add the checkpoint callback\n",
    "    progress_bar_refresh_rate=1,\n",
    "    log_every_n_steps=1  # Log metrics every step\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(classifier)\n"
   ],
   "id": "49c4e5d6f46f3f06"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs/"
   ],
   "id": "be727aefe3b8073a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Test the model\n",
    "trainer.test()"
   ],
   "id": "778377282eab7dde"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "'''\n",
    "TODO:\n",
    "1. check tensorboard here in the cell directly \n",
    "2. evaluate the evaluate code as the checkpointing and reloading of that model is not as straightforward as I thought it would be \n",
    "'''"
   ],
   "id": "dadf7840d0a71ad9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 5. Saving model & Evaluating Results",
   "id": "f2232fa60d92bde6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def load_model_from_checkpoint(checkpoint_path):\n",
    "    # Load the checkpoint\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "\n",
    "    # Create a new instance of your model\n",
    "    finetuned_model = RobustResnet()  # Initialize with the required parameters\n",
    "\n",
    "    # Load the model weights from the checkpoint\n",
    "    finetuned_model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    finetuned_model.eval()\n",
    "    \n",
    "    return finetuned_model\n",
    "\n",
    "def evaluate_model(checkpoint_path, original_model, test_dataloader, device=DEVICE):\n",
    "    # Load the fine-tuned model from the checkpoint\n",
    "    fine_tuned_model = load_model_from_checkpoint(checkpoint_path)\n",
    "\n",
    "    # Move models to the appropriate device\n",
    "    original_model.to(device)\n",
    "    fine_tuned_model.to(device)\n",
    "\n",
    "    # Initialize accuracy metrics\n",
    "    original_acc = Accuracy()\n",
    "    fine_tuned_acc = Accuracy()\n",
    "\n",
    "    # Evaluate the original model\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            images, labels = batch\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Original model predictions\n",
    "            original_logits = original_model(images)\n",
    "            original_acc(original_logits, labels)\n",
    "\n",
    "            # Fine-tuned model predictions\n",
    "            fine_tuned_logits = fine_tuned_model(images)\n",
    "            fine_tuned_acc(fine_tuned_logits, labels)\n",
    "\n",
    "    # Calculate accuracies\n",
    "    original_accuracy = original_acc.compute()\n",
    "    fine_tuned_accuracy = fine_tuned_acc.compute()\n",
    "\n",
    "    print(f'Evaluation Original Model Accuracy: {original_accuracy:.4f}')\n",
    "    print(f'Evaluation Fine-Tuned Model Accuracy: {fine_tuned_accuracy:.4f}')\n",
    "\n",
    "    return original_accuracy, fine_tuned_accuracy"
   ],
   "id": "eed147733535b4b9",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
