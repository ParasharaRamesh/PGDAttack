{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# TODOs: (before 13th sep ) (next friday)\n",
    "\n",
    "1. Need to create a balanced dataloader and save it all in google drive along with perturbations + augmentations, dataloader will just load from google drive using os. Use train set and from val set create test set also. (tonight). save locally also which can just be uploaded as is. (maybe run over night if needed) (by 7th)\n",
    "2. pytorch calls validation step first , maybe have to change the val epochs to batch size instead (when is it called, why is grad_fn not available)\n",
    " (check from here -> https://github.com/Lightning-AI/pytorch-lightning/issues/13948)\n",
    "3.understand a bit more about what zero grad and loss backward doesinternall first (by 8th)\n",
    "4. maybe don't use pytorch lightning at all,  just use pytorch entirely to control stuff (by 11th)\n"
   ],
   "id": "af1eef554c6a15af"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# About\n",
    "\n",
    "This notebook attempts to finetune a resnet model to be more Robust by leveraging Projected Gradient Descent (PGD) in the two different ways:\n",
    "1. include it as a part of the dataset\n",
    "2. include the projected gradient as well while training along with the regular gradient from the loss function "
   ],
   "id": "9befb1ddab3e4144"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 0. Importing required libraries\n",
   "id": "876d5185378e4464"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T06:12:52.506197Z",
     "start_time": "2024-09-06T06:12:52.480798Z"
    }
   },
   "cell_type": "code",
   "source": "# !pip install lightning[extra]",
   "id": "d359d5eb64ade957",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-06T06:21:45.934184Z",
     "start_time": "2024-09-06T06:21:45.902927Z"
    }
   },
   "source": [
    "import os\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "import random\n",
    "from datasets import load_dataset\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.auto import tqdm\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from torchmetrics import Accuracy\n",
    "from torchvision.transforms import Compose\n",
    "from torchvision.transforms import ToTensor\n",
    "from tqdm import tqdm\n",
    "import copy"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1. Setup",
   "id": "5f6951e549aa8a2c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T06:13:33.374223Z",
     "start_time": "2024-09-06T06:13:31.848283Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Manual seed for reproducibility\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Params for PGD\n",
    "ALPHA = 2/255\n",
    "STEPS = 20\n",
    "EPSILON = 8/255\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"device: {DEVICE}\")\n",
    "\n",
    "# model = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "# print(\"initialized the model\")"
   ],
   "id": "8e43067e9bcd293c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Creating a finetuning dataset\n",
    "\n",
    "Idea is to create a balanced finetuning dataset which is run only once to be saved onto the disk and then from there we can just create dataloaders on that for finetuning"
   ],
   "id": "42b55ed2273fd35d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.1 Reusing the same PGD attacker class from before",
   "id": "baeda959f22ce163"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T06:13:33.389997Z",
     "start_time": "2024-09-06T06:13:33.374223Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ResnetPGDAttacker:\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        The PGD attack on Resnet model.\n",
    "        :param model: The resnet model on which we perform the attack\n",
    "        :param dataloader: The dataloader loading the input data on which we perform the attack\n",
    "        '''\n",
    "        self.model = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.adv_images = []\n",
    "        self.labels = []\n",
    "        self.eps = 0\n",
    "        self.alpha = 0\n",
    "        self.steps = 0\n",
    "        self.acc = 0\n",
    "        self.adv_acc = 0\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "        # Nullify gradient for model params\n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def pgd_attack(self, image, label, eps=None, alpha=None, steps=None):\n",
    "        '''\n",
    "        Create adversarial images for given batch of images and labels\n",
    "\n",
    "        :param image: Batch of input images on which we perform the attack, size (BATCH_SIZE, 3, 224, 224)\n",
    "        :param label: Batch of input labels on which we perform the attack, size (BATCH_SIZE)\n",
    "        :return: Adversarial images for the given input images\n",
    "        '''\n",
    "        if eps is None:\n",
    "            eps = self.eps\n",
    "        if alpha is None:\n",
    "            alpha = self.alpha\n",
    "        if steps is None:\n",
    "            steps = self.steps\n",
    "\n",
    "        images = image.clone().detach().to(self.device)\n",
    "        adv_images = images.clone()\n",
    "        labels = label.clone().detach().to(self.device)\n",
    "\n",
    "        # Starting at a uniformly random point within the eps ball\n",
    "        random_noise = torch.zeros_like(adv_images).uniform_(-eps, eps)\n",
    "        adv_images = adv_images + random_noise\n",
    "\n",
    "        for _ in range(steps):\n",
    "            # Enable gradient tracking for adversarial images\n",
    "            adv_images.requires_grad = True\n",
    "\n",
    "            # Get model predictions and apply softmax\n",
    "            outputs = self.model(adv_images).softmax(1)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = self.loss_fn(outputs, labels)\n",
    "\n",
    "            # Compute gradient wrt images\n",
    "            grad = torch.autograd.grad(\n",
    "                loss, adv_images, retain_graph=False, create_graph=False\n",
    "            )[0]\n",
    "            adv_images = adv_images.detach()\n",
    "\n",
    "            # Gradient update\n",
    "            adv_images = adv_images + alpha * grad.sign()  # Update adversarial images using the sign of the gradient\n",
    "\n",
    "            # Projection step\n",
    "            # Clamping the adversarial images to ensure they are within the Lâˆž ball of eps radius of original image\n",
    "            adv_images = torch.clamp(adv_images, images - eps, images + eps)\n",
    "\n",
    "            adv_images = adv_images.detach()\n",
    "\n",
    "        return adv_images  # Return the generated adversarial images\n",
    "\n"
   ],
   "id": "13d4022de5445e2",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.2 Creating a finetuned dataset and saving it in disk",
   "id": "aa8168ce97b94c82"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T06:21:16.941809Z",
     "start_time": "2024-09-06T06:18:36.273961Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class FineTuneDatasetGenerator:\n",
    "    def __init__(self, num_classes=1000, num_images_per_class=4, num_transforms=2, num_perturbations=2,\n",
    "                 batch_size=2,\n",
    "                 save_path=\"./dataset\"\n",
    "                 ):\n",
    "        self.num_classes = num_classes\n",
    "        self.num_images_per_class = num_images_per_class\n",
    "        self.num_transforms = num_transforms\n",
    "        self.num_perturbations = num_perturbations\n",
    "        self.save_path = save_path\n",
    "        self.images_per_class = {i: [] for i in range(self.num_classes)}  # Dictionary to store images for each class\n",
    "\n",
    "        weights = ResNet50_Weights.DEFAULT\n",
    "        self.resnet_transform = weights.transforms()  #PIL -> tensor\n",
    "\n",
    "        self.transformations = [\n",
    "            transforms.Compose([\n",
    "                transforms.RandomRotation(15)\n",
    "            ]),\n",
    "            transforms.Compose([\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomRotation(15)\n",
    "            ]),\n",
    "            transforms.Compose([\n",
    "                transforms.RandomVerticalFlip(),\n",
    "                transforms.RandomRotation(15)\n",
    "            ])\n",
    "        ]\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.pgd_attacker = ResnetPGDAttacker()\n",
    "\n",
    "        self.ds = load_dataset(\"ILSVRC/imagenet-1k\", split=\"train\", streaming=True, trust_remote_code=True)\n",
    "        self.ds = self.ds.shuffle()\n",
    "        print(f\"Fine Tune Dataset Generator has been initialized.\")\n",
    "\n",
    "    def get_n_images_for_each_label(self):\n",
    "        for label in tqdm(range(self.num_classes), desc=\"Fetching images for each class\"):\n",
    "            # Define the filter function for the current class label\n",
    "            def filter_class(example):\n",
    "                return example['label'] == label and example['image'].mode == 'RGB'\n",
    "\n",
    "            # Load the dataset and filter for the current class\n",
    "            ds = copy.deepcopy(self.ds)\n",
    "            ds = ds.filter(filter_class)\n",
    "\n",
    "            # Use take to get the desired number of images\n",
    "            ds = ds.take(self.num_images_per_class)\n",
    "            # dl = DataLoader(ds, batch_size=self.num_images_per_class) # cant use if it is PIL\n",
    "\n",
    "            i = 0\n",
    "            for data in ds:\n",
    "                img = data[\"image\"]\n",
    "                tensor_img = self.resnet_transform(img)\n",
    "                self.images_per_class[label].append(\n",
    "                    (img, tensor_img)  #original PIL, tensor version\n",
    "                )\n",
    "                i += 1\n",
    "                if i == self.num_images_per_class:\n",
    "                    break\n",
    "\n",
    "            # approach doesnt work\n",
    "            # for batch in dl:\n",
    "            #     images = batch['image']\n",
    "            # \n",
    "            #     # Store images in the dictionary; use the first label from the batch\n",
    "            #     for img in images:\n",
    "            #         tensor_img = self.resnet_transform(img)\n",
    "            #         self.images_per_class[label].append(\n",
    "            #             (img, tensor_img) #original PIL, tensor version\n",
    "            #         )\n",
    "\n",
    "            print(f\"Collected {len(self.images_per_class[label])} images for class {label}.\")\n",
    "\n",
    "    def augment_images(self):\n",
    "        for label, images in tqdm(self.images_per_class.items(), desc=\"Augmenting images for class\"):\n",
    "            for data in images:\n",
    "                if type(data) == tuple:\n",
    "                    img = data[0]\n",
    "                    for _ in range(self.num_transforms):\n",
    "                        transformed_img = random.choice(self.transformations)(img)\n",
    "                        transformed_img = self.resnet_transform(transformed_img)\n",
    "                        self.images_per_class[label].append(transformed_img)  #directly as tensor\n",
    "\n",
    "        # use the resnet transform on all images now\n",
    "        for label, images in tqdm(self.images_per_class.items(),\n",
    "                                  desc=\"Using resnet transforms for all images + augmentations\"):\n",
    "            tensor_images = []\n",
    "            for data in images:\n",
    "                if type(data) == tuple:\n",
    "                    tensor_images.append(data[1])  # take the tensor version\n",
    "                else:\n",
    "                    tensor_images.append(data)  # already is a tensor\n",
    "\n",
    "            self.images_per_class[label] = tensor_images\n",
    "\n",
    "        print(f\"{self.num_transforms} Augmentations done for each img\")\n",
    "\n",
    "    def perturb_images(self):\n",
    "        for label, images in tqdm(self.images_per_class.items(), desc=\"Generating perturbations for class\"):\n",
    "            # Process images in batches\n",
    "            for _ in range(self.num_perturbations):\n",
    "                for i in range(0, len(images), self.batch_size):\n",
    "                    batch = list(\n",
    "                        map(\n",
    "                            lambda img: img.to(\"cpu\"),\n",
    "                            images[i:i+self.batch_size]\n",
    "                        )\n",
    "                    )\n",
    "                    # Get a batch of images and create corresponding labels\n",
    "                    img_batch = torch.stack(batch)\n",
    "                    label_batch = torch.tensor([label] * len(img_batch))  # Repeat the label for the batch\n",
    "\n",
    "                    # Generate random parameters for PGD attack\n",
    "                    random_eps = random.uniform(0.01, 0.3)\n",
    "                    random_alpha = random.uniform(0.01, 0.1)\n",
    "                    random_steps = random.randint(15, 20)\n",
    "\n",
    "                    # Perform the PGD attack\n",
    "                    perturbed_images = self.pgd_attacker.pgd_attack(img_batch, label_batch,\n",
    "                                                                    eps=random_eps,\n",
    "                                                                    alpha=random_alpha,\n",
    "                                                                    steps=random_steps)\n",
    "\n",
    "                    # Store the perturbed images\n",
    "                    for perturbed_image in perturbed_images:\n",
    "                        self.images_per_class[label].append(perturbed_image)\n",
    "\n",
    "        print(f\"{self.num_perturbations} perturbations done for each img\")\n",
    "\n",
    "    def save_images(self):\n",
    "        # Create the save directory if it doesn't exist\n",
    "        os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "        for label, images in tqdm(self.images_per_class.items()):\n",
    "            for idx, image in enumerate(images):\n",
    "                save_file = os.path.join(self.save_path, f\"class_{label}_img_{idx}.pt\")\n",
    "                torch.save(image, save_file)\n",
    "\n",
    "        print(f\"All images saved to {self.save_path}.\")\n",
    "\n",
    "    def generate(self):\n",
    "        self.get_n_images_for_each_label()\n",
    "        self.augment_images()\n",
    "        self.perturb_images()\n",
    "        self.save_images()\n",
    "\n",
    "\n",
    "# Example usage\n",
    "fine_tune_gen = FineTuneDatasetGenerator(num_classes=2, num_images_per_class=1)\n",
    "fine_tune_gen.generate()"
   ],
   "id": "13682654a99a1177",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine Tune Dataset Generator has been initialized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching images for each class:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:36<00:36, 36.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 1 images for class 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching images for each class: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:46<00:00, 23.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 1 images for class 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Augmenting images for class: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 90.51it/s]\n",
      "Using resnet transforms for all images + augmentations: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Augmentations done for each img\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating perturbations for class: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [01:22<00:00, 41.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 perturbations done for each img\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:04<00:00,  2.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All images saved to ./dataset.\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.3 Creating a Pytorch dataset and dataloaders over the saved data in the disk",
   "id": "24e3bcde543f5989"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T06:27:24.414564Z",
     "start_time": "2024-09-06T06:26:49.236973Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "\n",
    "class FineTunedDataset(Dataset):\n",
    "    def __init__(self, root_dir):\n",
    "        self.root_dir = root_dir\n",
    "        self.image_files = os.listdir(root_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load the image from the file\n",
    "        image_file = self.image_files[idx]\n",
    "        image = torch.load(os.path.join(self.root_dir, image_file))\n",
    "\n",
    "        # Extract label from the filename\n",
    "        label = int(os.path.basename(image_file).split(\"_\")[1])  # e.g., \"class_0_img_3.pt\" -> label = 0\n",
    "\n",
    "        return image, label\n",
    "\n",
    "def get_dataloaders(root_dir, batch_size, train_val_test_split):\n",
    "    dataset = FineTunedDataset(root_dir)\n",
    "\n",
    "    train_size = int(len(dataset) * train_val_test_split[0])\n",
    "    val_size = int(len(dataset) * train_val_test_split[1])\n",
    "    test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "    train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "path = \"C:\\\\Parashara\\\\Projects\\\\NUS projects\\\\Sem3\\\\Trustworthy ML\\\\Assignment 1\\\\PGDAttack\\\\dataset\"\n",
    "train_loader, val_loader, test_loader = get_dataloaders(path, batch_size=2, train_val_test_split=[0.8, 0.1, 0.1])\n",
    "\n",
    "for batch in train_loader:\n",
    "    break"
   ],
   "id": "7eeddfed982403a2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paras\\AppData\\Local\\Temp\\ipykernel_16792\\808235905.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  image = torch.load(os.path.join(self.root_dir, image_file))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[11], line 41\u001B[0m\n\u001B[0;32m     38\u001B[0m train_loader, val_loader, test_loader \u001B[38;5;241m=\u001B[39m get_dataloaders(path, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m, train_val_test_split\u001B[38;5;241m=\u001B[39m[\u001B[38;5;241m0.8\u001B[39m, \u001B[38;5;241m0.1\u001B[39m, \u001B[38;5;241m0.1\u001B[39m])\n\u001B[0;32m     40\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m batch \u001B[38;5;129;01min\u001B[39;00m train_loader:\n\u001B[1;32m---> 41\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[11], line 41\u001B[0m\n\u001B[0;32m     38\u001B[0m train_loader, val_loader, test_loader \u001B[38;5;241m=\u001B[39m get_dataloaders(path, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m, train_val_test_split\u001B[38;5;241m=\u001B[39m[\u001B[38;5;241m0.8\u001B[39m, \u001B[38;5;241m0.1\u001B[39m, \u001B[38;5;241m0.1\u001B[39m])\n\u001B[0;32m     40\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m batch \u001B[38;5;129;01min\u001B[39;00m train_loader:\n\u001B[1;32m---> 41\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_310_64.pyx:1187\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_310_64.SafeCallWrapper.__call__\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_310_64.pyx:627\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_310_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_310_64.pyx:937\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_310_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_310_64.pyx:928\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_310_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_310_64.pyx:585\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_310_64.PyDBFrame.do_wait_suspend\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m~\\AppData\\Local\\JetBrains\\PyCharm 2024.1.4\\plugins\\python\\helpers\\pydev\\pydevd.py:1201\u001B[0m, in \u001B[0;36mPyDB.do_wait_suspend\u001B[1;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[0m\n\u001B[0;32m   1198\u001B[0m         from_this_thread\u001B[38;5;241m.\u001B[39mappend(frame_id)\n\u001B[0;32m   1200\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_threads_suspended_single_notification\u001B[38;5;241m.\u001B[39mnotify_thread_suspended(thread_id, stop_reason):\n\u001B[1;32m-> 1201\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_do_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msuspend_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrom_this_thread\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\JetBrains\\PyCharm 2024.1.4\\plugins\\python\\helpers\\pydev\\pydevd.py:1216\u001B[0m, in \u001B[0;36mPyDB._do_wait_suspend\u001B[1;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[0m\n\u001B[0;32m   1213\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_mpl_hook()\n\u001B[0;32m   1215\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocess_internal_commands()\n\u001B[1;32m-> 1216\u001B[0m         \u001B[43mtime\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0.01\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1218\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcancel_async_evaluation(get_current_thread_id(thread), \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mid\u001B[39m(frame)))\n\u001B[0;32m   1220\u001B[0m \u001B[38;5;66;03m# process any stepping instructions\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### (TODO) can remove from here !1.1 Loading the datasets",
   "id": "547319e9ec62f2d9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-05T04:31:43.613241Z",
     "start_time": "2024-09-05T04:31:43.602867Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess_img(example):\n",
    "    weights = ResNet50_Weights.DEFAULT\n",
    "    preprocess = weights.transforms()\n",
    "    example['image'] = preprocess(example['image'])\n",
    "    return example\n",
    "\n",
    "def get_dataloader(split: str, batch_num: int, batch_size: int):\n",
    "    '''\n",
    "        \n",
    "    :param split: can be either train, test or validation \n",
    "    :return: \n",
    "    '''\n",
    "    print(f\"Loading {split} ILSVRC/imagenet-1k dataset...\")\n",
    "    ds = load_dataset(\"ILSVRC/imagenet-1k\", split=split, streaming=True, trust_remote_code=True)\n",
    "    \n",
    "    # Filter out grayscale images\n",
    "    ds = ds.filter(lambda example: example['image'].mode == 'RGB')\n",
    "    \n",
    "    # Preprocess function will be applied to images on-the-fly whenever they are being accessed in the loop\n",
    "    ds = ds.map(preprocess_img)\n",
    "    ds = ds.shuffle(seed=SEED)\n",
    "    \n",
    "    # Only take desired portion of dataset\n",
    "    ds = ds.take(batch_num * batch_size)\n",
    "    print(f\"Creating dataloader with {batch_num} batches for split {split} each with size {batch_size}\")\n",
    "    return DataLoader(ds, batch_size=batch_size)\n",
    "    "
   ],
   "id": "1e9fafc8606bddd2",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-03T15:38:23.708093Z",
     "start_time": "2024-09-03T15:38:04.788423Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# TODO. Can use this for filtering\n",
    "def chumma(example):\n",
    "    # print(f\"example: {example}\")\n",
    "    # print(f\"example['label']: {example['label']} -> {type(example['label'])}\")\n",
    "    return example['label'] == 916\n",
    "\n",
    "ds = load_dataset(\"ILSVRC/imagenet-1k\", split=\"train\", streaming=True, trust_remote_code=True)\n",
    "ds = ds.filter(chumma)\n",
    "# ds = ds.filter(lambda example: example['image'].mode == 'RGB')\n",
    "# ds = ds.filter(lambda example: example['label'] == 726)\n",
    "ds = ds.map(preprocess_img)\n",
    "ds = ds.take(2 * 2)\n",
    "dl = DataLoader(ds, batch_size=2)\n",
    "\n",
    "for batch in dl:\n",
    "    image, label = batch[\"image\"], batch[\"label\"]\n",
    "    print(label)\n",
    "    break"
   ],
   "id": "7feb09cf8eaaa1b8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([916, 916])\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1.2 My Resnet PGD Attacker",
   "id": "2c84e145464783a6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-05T04:36:23.259977Z",
     "start_time": "2024-09-05T04:36:23.244030Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ResnetPGDAttacker:\n",
    "    def __init__(self, model=None, device=DEVICE):\n",
    "        '''\n",
    "        The PGD attack on Resnet model.\n",
    "        :param model: The resnet model on which we perform the attack\n",
    "        :param dataloader: The dataloader loading the input data on which we perform the attack\n",
    "        '''\n",
    "        self.device = device\n",
    "        \n",
    "        if model == None:\n",
    "            print(f\"Creating new model\")\n",
    "            self.model = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "        else:\n",
    "            print(f\"Using existing model\")\n",
    "            self.model = model     \n",
    "        self.model.to(self.device)\n",
    "        \n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.eps = 0\n",
    "        self.alpha = 0\n",
    "        self.steps = 0\n",
    "        \n",
    "        # Nullify gradient for model params\n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def pgd_attack(self, image, label, eps=None, alpha=None, steps=None):\n",
    "        '''\n",
    "        Create adversarial images for given batch of images and labels\n",
    "\n",
    "        :param image: Batch of input images on which we perform the attack, size (BATCH_SIZE, 3, 224, 224)\n",
    "        :param label: Batch of input labels on which we perform the attack, size (BATCH_SIZE)\n",
    "        :return: Adversarial images for the given input images\n",
    "        '''\n",
    "        if eps is None:\n",
    "            eps = self.eps\n",
    "        if alpha is None:\n",
    "            alpha = self.alpha\n",
    "        if steps is None:\n",
    "            steps = self.steps\n",
    "\n",
    "        images = image.clone().to(self.device)\n",
    "        adv_images = image.clone().to(self.device)\n",
    "        labels = label.clone().to(self.device)\n",
    "\n",
    "        # Starting at a uniformly random point within the eps ball\n",
    "        random_noise = torch.zeros_like(adv_images).uniform_(-eps, eps)\n",
    "        adv_images = adv_images + random_noise\n",
    "        \n",
    "        # Enable gradient tracking for adversarial images\n",
    "        adv_images.requires_grad = True\n",
    "\n",
    "        for _ in range(steps):\n",
    "            # Get model predictions and apply softmax\n",
    "            outputs = self.model(adv_images).softmax(1)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = self.loss_fn(outputs, labels)\n",
    "                        \n",
    "            assert loss.requires_grad == True, f\"loss should be requires_grad {loss}.\"\n",
    "            assert adv_images.requires_grad == True, \"adv_images should be requires_grad\"\n",
    "            \n",
    "            # Compute gradient wrt images\n",
    "            grad = torch.autograd.grad(\n",
    "                loss, adv_images, retain_graph=False, create_graph=False\n",
    "            )[0]\n",
    "\n",
    "            # Gradient update\n",
    "            adv_images = adv_images + alpha * grad.sign()  # Update adversarial images using the sign of the gradient\n",
    "\n",
    "            # Projection step\n",
    "            # Clamping the adversarial images to ensure they are within the Lâˆž ball of eps radius of original image\n",
    "            adv_images = torch.clamp(adv_images, images - eps, images + eps)\n",
    "\n",
    "        #detaching only in the end\n",
    "        adv_images = adv_images.detach()\n",
    "        return adv_images  # Return the generated adversarial images\n"
   ],
   "id": "29fb7354c84af91b",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 2. Functions related to generating & saving + fetching saved data from a location\n",
    "\n",
    "(TODO) : maybe this is not needed as I am creating a dataset directly!"
   ],
   "id": "4651ce982318861a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-05T04:36:24.320211Z",
     "start_time": "2024-09-05T04:36:24.285772Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def fetch_perturbed_images_from_location(location: str):\n",
    "    \"\"\"\n",
    "    Fetch and concatenate all perturbed images and labels from the specified directory.\n",
    "\n",
    "    :param location: The directory where the .pt files are saved.\n",
    "    :return: Tuple of concatenated adversarial images and labels.\n",
    "    \"\"\"\n",
    "    adv_images = []\n",
    "    labels = []\n",
    "\n",
    "    # Iterate through all .pt files in the specified directory\n",
    "    for file_name in os.listdir(location):\n",
    "        if file_name.endswith(\".pt\"):\n",
    "            file_path = os.path.join(location, file_name)\n",
    "            data = torch.load(file_path)\n",
    "            adv_images.append(data['adv_images'])\n",
    "            labels.append(data['labels'])\n",
    "\n",
    "    # Concatenate all adversarial images and labels\n",
    "    adv_images = torch.cat(adv_images)\n",
    "    labels = torch.cat(labels)\n",
    "\n",
    "    return adv_images, labels\n",
    "\n",
    "def generate_adv_images(dataloader, store: bool = False, store_dir: str = 'adv_images'):\n",
    "    \"\"\"\n",
    "    Generate adversarial images using the PGD method and save them in a directory.\n",
    "\n",
    "    :param dataloader: dataloader of images, labels.\n",
    "    :param model: The ResNet model used for generating adversarial images.\n",
    "    :param store: Boolean flag to indicate whether to save the generated images.\n",
    "    :param store_dir: The directory where the adversarial images and labels will be saved.\n",
    "    :return: Tuple of adversarial images and their corresponding labels.\n",
    "    \"\"\"    \n",
    "    # Initialize the attacker\n",
    "    attacker = ResnetPGDAttacker()\n",
    "\n",
    "    # Generate adversarial images\n",
    "    adv_images = []\n",
    "    labels = []\n",
    "    for batch in tqdm(dataloader):\n",
    "        img_batch, label_batch = batch['image'], batch['label']\n",
    "        adv_images_batch = attacker.pgd_attack(img_batch, label_batch, steps = random.randint(5, STEPS)) #randomly take 5->20 steps\n",
    "        adv_images.append(adv_images_batch)\n",
    "        labels.append(label_batch)\n",
    "\n",
    "    # Concatenate all adversarial images\n",
    "    adv_images = torch.cat(adv_images)\n",
    "    labels = torch.cat(labels)\n",
    "\n",
    "    if store:\n",
    "        # Create the store directory if it doesn't exist\n",
    "        os.makedirs(store_dir, exist_ok=True)\n",
    "    \n",
    "        # Save the adversarial images and labels in multiple files\n",
    "        for j in range(0, len(adv_images), 1000):  # Save in batches of 1000\n",
    "            start = j\n",
    "            end = min(j + 1000, len(adv_images))\n",
    "            file_path = os.path.join(store_dir, f\"adv_images_{start}-{end-1}.pt\")\n",
    "            torch.save({'adv_images': adv_images[start:end], 'labels': labels[start:end]}, file_path)\n",
    "            print(f\"Adversarial images saved at: {file_path}\")\n",
    "        \n",
    "    return store_dir, adv_images, labels"
   ],
   "id": "1e038a305a7bcbe0",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3. Creating the fine-tuning dataset",
   "id": "daebe3fda5c80640"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-05T04:36:25.636784Z",
     "start_time": "2024-09-05T04:36:25.605964Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class RobustDataLoader(DataLoader):\n",
    "    def __init__(self, dataset, split, batch_size=1, batch_num=1, num_perturbations=2, num_transformations=1,\n",
    "                 device=DEVICE,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        Robust DataLoader that includes original, perturbed, and augmented images.\n",
    "\n",
    "        :param dataset: Dataset containing original images and labels.\n",
    "        :param model: The ResNet model used for generating adversarial images.\n",
    "        :param num_perturbations: Number of perturbations to generate for each original image.\n",
    "        :param num_transformations: Number of transformations to apply to each image.\n",
    "        :param kwargs: Additional arguments to be passed to the parent class constructor.\n",
    "        \"\"\"\n",
    "        super().__init__(dataset, batch_size=batch_size,\n",
    "                         **kwargs)  # self.dataset & self.batch_size is defined inside of this       \n",
    "        self.device = device\n",
    "        self.split = split\n",
    "        self.num_perturbations = num_perturbations\n",
    "        self.num_transformations = num_transformations\n",
    "        self.pgd_attacker = ResnetPGDAttacker()\n",
    "        self.batch_num = batch_num\n",
    "        self.dataset_size = self.batch_num * self.batch_size\n",
    "        \n",
    "        if self.split == \"test\": \n",
    "            self.per_yield_limit = self.batch_size\n",
    "            print(f\"setting per_yield_limit to {self.per_yield_limit}, which is the same as batch size as it is in test mode\")\n",
    "        else:\n",
    "            self.per_yield_limit = self.batch_size * (1 + self.num_transformations + self.num_perturbations)\n",
    "            print(f\"setting per_yield_limit to {self.per_yield_limit}, as there are additional #{self.num_transformations} transformations & #{self.num_perturbations} perturbations\")\n",
    "            \n",
    "\n",
    "        # Define transformations\n",
    "        self.transformations = [\n",
    "            transforms.Compose([\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "            ]),\n",
    "            transforms.Compose([\n",
    "                transforms.RandomVerticalFlip(),\n",
    "            ]),\n",
    "            transforms.Compose([\n",
    "                transforms.RandomRotation(20),\n",
    "            ]),\n",
    "            transforms.Compose([\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomRotation(20),\n",
    "                transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "            ]),\n",
    "            transforms.Compose([\n",
    "                transforms.RandomVerticalFlip(),\n",
    "                transforms.RandomRotation(20),\n",
    "                transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "            ])\n",
    "        ]\n",
    "        print(\n",
    "            f\"RobustDataLoader initialized with split={split}, batch_num={batch_num}, batch_size={self.batch_size}, num_perturbations={self.num_perturbations}, num_transformations={self.num_transformations}, per_yield_limit={self.per_yield_limit}\")\n",
    "\n",
    "    def perform_pgd(self, image, label):\n",
    "        random_eps = random.uniform(0.01, 0.3)\n",
    "        random_alpha = random.uniform(0.01, 0.1)\n",
    "        random_steps = random.randint(15, 20)\n",
    "        perturbed_image = self.pgd_attacker.pgd_attack(image.unsqueeze(0), label.unsqueeze(0), eps=random_eps,\n",
    "                                                       alpha=random_alpha, steps=random_steps)\n",
    "        return perturbed_image.squeeze(0).to(self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        '''\n",
    "        For each image, we do T transformations and P perturbations and retain the original image.\n",
    "        \n",
    "        :return: \n",
    "        '''\n",
    "        return (self.batch_num * self.batch_size) * (1 + self.num_transformations + self.num_perturbations)\n",
    "\n",
    "    def __iter__(self):\n",
    "        collected_images = []\n",
    "        collected_labels = []\n",
    "\n",
    "        for datapoint in self.dataset:\n",
    "            image = datapoint['image'].to(self.device)\n",
    "            label = torch.tensor(datapoint['label']).to(self.device)\n",
    "\n",
    "            # Collect original images and labels\n",
    "            collected_images.append(image)\n",
    "            collected_labels.append(label)\n",
    "\n",
    "            if self.split != \"test\":\n",
    "                # Generate and collect perturbations\n",
    "                for _ in range(self.num_perturbations):\n",
    "                    perturbed_image = self.perform_pgd(image, label)\n",
    "                    collected_images.append(perturbed_image)\n",
    "                    collected_labels.append(label)\n",
    "    \n",
    "                # Generate and collect transformations\n",
    "                for _ in range(self.num_transformations):\n",
    "                    transformation_index = random.randint(0, len(self.transformations) - 1)\n",
    "                    transformed_image = self.transformations[transformation_index](image).to(self.device)\n",
    "                    collected_images.append(transformed_image)\n",
    "                    collected_labels.append(label)\n",
    "\n",
    "            # Check if we have collected enough images for the batch\n",
    "            if len(collected_images) == self.per_yield_limit:\n",
    "                # Yield the batch\n",
    "                yield {\n",
    "                    \"image\": torch.stack(collected_images[:self.per_yield_limit]).to(self.device),\n",
    "                    \"label\": torch.tensor(collected_labels[:self.per_yield_limit]).to(self.device)\n",
    "                }\n",
    "\n",
    "                # Clear the lists after yielding\n",
    "                collected_images.clear()\n",
    "                collected_labels.clear()\n"
   ],
   "id": "1e6d83ded5e4d4f0",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-05T04:36:26.574170Z",
     "start_time": "2024-09-05T04:36:26.542789Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_dataloader(split, batch_num=1, batch_size=1, num_perturbations=1, num_transformations=1):\n",
    "    # Load the dataset directly using load_dataset\n",
    "    ds = load_dataset(\"ILSVRC/imagenet-1k\", split=split, streaming=True, trust_remote_code=True)\n",
    "\n",
    "    # Preprocess the dataset\n",
    "    ds = ds.filter(lambda example: example['image'].mode == 'RGB')\n",
    "    ds = ds.map(preprocess_img)\n",
    "    ds = ds.shuffle(seed=SEED)\n",
    "    ds = ds.take(batch_num * batch_size)\n",
    "    print(f\"loaded dataset of {batch_num * batch_size} images & labels from split {split}\")\n",
    "\n",
    "    # Create the RobustDataLoader\n",
    "    data_loader = RobustDataLoader(ds,\n",
    "                                   split=split,\n",
    "                                   num_perturbations=num_perturbations,\n",
    "                                   num_transformations=num_transformations,\n",
    "                                   batch_size=batch_size,\n",
    "                                   batch_num=batch_num\n",
    "                                   )\n",
    "    print(f\"Instantiated dataloader for split {split}, with {len(data_loader)} image, label pairs\")\n",
    "    assert len(data_loader) == batch_num * batch_size * (\n",
    "            1 + num_transformations + num_perturbations), f\"Length of the dataset should have been {batch_num * batch_size * (1 + num_transformations) + num_perturbations}\"\n",
    "    return data_loader\n"
   ],
   "id": "2529e44c8e523535",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.1 Testing Data loaders",
   "id": "8452f74696292c0b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-03T10:30:02.203838Z",
     "start_time": "2024-09-03T10:29:39.111686Z"
    }
   },
   "cell_type": "code",
   "source": [
    "loader = create_dataloader(\"train\", batch_num=2, batch_size=1)\n",
    "\n",
    "for i, batch in enumerate(loader):\n",
    "    print(f\"batch {i}\")\n",
    "    img, label = batch['image'], batch['label']\n",
    "    print(f\"image shape: {img.shape}\")\n",
    "    print(f\"label shape: {label.shape}, labels: {label}\")\n",
    "    print(\"-\"* 30)"
   ],
   "id": "93e92c7aa9b7fffe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded dataset of 2 images & labels from split train\n",
      "Creating new model\n",
      "setting per_yield_limit to 3, as there are additional #1 transformations & #1 perturbations\n",
      "RobustDataLoader initialized with split=train, batch_num=2, batch_size=1, num_perturbations=1, num_transformations=1, per_yield_limit=3\n",
      "Instantiated dataloader for split train, with 6 image, label pairs\n",
      "batch 0\n",
      "image shape: torch.Size([3, 3, 224, 224])\n",
      "label shape: torch.Size([3]), labels: tensor([417, 417, 417], device='cuda:0')\n",
      "------------------------------\n",
      "batch 1\n",
      "image shape: torch.Size([3, 3, 224, 224])\n",
      "label shape: torch.Size([3]), labels: tensor([476, 476, 476], device='cuda:0')\n",
      "------------------------------\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-03T10:30:23.117198Z",
     "start_time": "2024-09-03T10:30:02.203838Z"
    }
   },
   "cell_type": "code",
   "source": [
    "loader = create_dataloader(\"validation\", batch_num=2, batch_size=1)\n",
    "for batch in loader:\n",
    "    print(f\"val batch {i}\")\n",
    "    img, label = batch['image'], batch['label']\n",
    "    print(f\"image shape: {img.shape}\")\n",
    "    print(f\"label shape: {label.shape}\")\n",
    "    print(\"-\"* 30)\n"
   ],
   "id": "4ee1538f4d827db2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded dataset of 2 images & labels from split validation\n",
      "Creating new model\n",
      "setting per_yield_limit to 3, as there are additional #1 transformations & #1 perturbations\n",
      "RobustDataLoader initialized with split=validation, batch_num=2, batch_size=1, num_perturbations=1, num_transformations=1, per_yield_limit=3\n",
      "Instantiated dataloader for split validation, with 6 image, label pairs\n",
      "val batch 1\n",
      "image shape: torch.Size([3, 3, 224, 224])\n",
      "label shape: torch.Size([3])\n",
      "------------------------------\n",
      "val batch 1\n",
      "image shape: torch.Size([3, 3, 224, 224])\n",
      "label shape: torch.Size([3])\n",
      "------------------------------\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-03T10:30:43.630923Z",
     "start_time": "2024-09-03T10:30:23.117198Z"
    }
   },
   "cell_type": "code",
   "source": [
    "loader = create_dataloader(\"test\", batch_num=2, batch_size=1)\n",
    "for batch in loader:\n",
    "    print(f\"test batch {i}\")\n",
    "    img, label = batch['image'], batch['label']\n",
    "    print(f\"image shape: {img.shape}\")\n",
    "    print(f\"label shape: {label.shape}\")\n",
    "    print(\"-\"* 30)\n",
    "\n",
    "del loader"
   ],
   "id": "2b16508d38b65829",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded dataset of 2 images & labels from split test\n",
      "Creating new model\n",
      "setting per_yield_limit to 1, which is the same as batch size as it is in test mode\n",
      "RobustDataLoader initialized with split=test, batch_num=2, batch_size=1, num_perturbations=1, num_transformations=1, per_yield_limit=1\n",
      "Instantiated dataloader for split test, with 6 image, label pairs\n",
      "test batch 1\n",
      "image shape: torch.Size([1, 3, 224, 224])\n",
      "label shape: torch.Size([1])\n",
      "------------------------------\n",
      "test batch 1\n",
      "image shape: torch.Size([1, 3, 224, 224])\n",
      "label shape: torch.Size([1])\n",
      "------------------------------\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 4. Implementing Robust Resnet",
   "id": "f2b608c00b6103c6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-05T08:23:58.639844Z",
     "start_time": "2024-09-05T08:22:59.364660Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class RobustResnet(LightningModule):\n",
    "    def __init__(self, train_loader, val_loader, test_loader, learning_rate=1e-3):\n",
    "        super().__init__()\n",
    "        self.model = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.train_loader = train_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.learning_rate = learning_rate\n",
    "        self.train_acc = Accuracy(\"multiclass\", num_classes=1000)\n",
    "        self.val_acc = Accuracy(\"multiclass\", num_classes=1000)\n",
    "        self.test_acc = Accuracy(\"multiclass\", num_classes=1000)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, labels = batch['image'], batch['label']\n",
    "        logits = self.model(images)\n",
    "        ce_loss = self.loss_fn(logits, labels)\n",
    "        # pgd_loss = self.compute_pgd_loss(images, labels)\n",
    "        # loss = ce_loss + pgd_loss\n",
    "\n",
    "        self.train_acc(logits, labels)\n",
    "        self.log('train_ce_loss', ce_loss)\n",
    "        # self.log('train_pgd_loss', pgd_loss)\n",
    "        # self.log('train_loss', loss)\n",
    "        self.log('train_acc', self.train_acc, on_step=False, on_epoch=True)\n",
    "        # return loss\n",
    "        return ce_loss\n",
    "\n",
    "    # def validation_step(self, batch, batch_idx):\n",
    "    #     images, labels = batch['image'], batch['label']\n",
    "    #     logits = self.model(images)\n",
    "    #     ce_loss = self.loss_fn(logits, labels)\n",
    "    #     pgd_loss = self.compute_pgd_loss(images, labels)\n",
    "    #     loss = ce_loss + pgd_loss\n",
    "    #     self.val_acc(logits, labels)\n",
    "    #     self.log('val_ce_loss', ce_loss, on_step=False, on_epoch=True)\n",
    "    #     self.log('val_pgd_loss', pgd_loss, on_step=False, on_epoch=True)\n",
    "    #     self.log('val_loss', loss, on_step=False, on_epoch=True)\n",
    "    #     self.log('val_acc', self.val_acc, on_step=False, on_epoch=True)\n",
    "\n",
    "    #TODO. this wont work as the labels will be -1 in this case\n",
    "    # def test_step(self, batch, batch_idx):\n",
    "    #     images, labels = batch\n",
    "    #     logits = self.model(images)\n",
    "    #     ce_loss = F.cross_entropy(logits, labels)\n",
    "    #     pgd_loss = self.pgd_loss(images, labels)\n",
    "    #     loss = ce_loss + pgd_loss\n",
    "    #     acc = self.test_acc(logits, labels)\n",
    "    #     self.log('test_ce_loss', ce_loss)\n",
    "    #     self.log('test_pgd_loss', pgd_loss)\n",
    "    #     self.log('test_loss', loss)\n",
    "    #     self.log('test_acc', acc)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    def compute_pgd_loss(self, images, labels):\n",
    "        # randomize pgd params\n",
    "        eps = random.uniform(0.01, 0.3)\n",
    "        alpha = random.uniform(0.01, 0.1)\n",
    "        steps = random.randint(15, 20)\n",
    "        \n",
    "        # take a copy\n",
    "        adv_images = images.clone().detach().requires_grad_(True).to(images.device)\n",
    "        \n",
    "        # Starting at a uniformly random point within the eps ball\n",
    "        random_noise = torch.zeros_like(adv_images).uniform_(-eps, eps)\n",
    "        adv_images = adv_images + random_noise\n",
    "               \n",
    "        # do randomized PGD process and compute loss\n",
    "        for _ in range(steps):\n",
    "            # Get model predictions\n",
    "            outputs = self.model(adv_images)\n",
    "\n",
    "            # Calculate loss\n",
    "            pgd_loss = self.loss_fn(outputs, labels)\n",
    "\n",
    "            # Compute gradient wrt adversarial images\n",
    "            pgd_loss.backward(retain_graph=True)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # Gradient update\n",
    "                adv_images = adv_images + alpha * adv_images.grad.sign()  # Update adversarial images using the sign of the gradient\n",
    "    \n",
    "                # Projection step\n",
    "                # Clamping the adversarial images to ensure they are within the Lâˆž ball of eps radius of original image\n",
    "                adv_images = torch.clamp(adv_images, images - eps, images + eps)\n",
    "            \n",
    "            # Clear the gradients for the next step\n",
    "            adv_images.grad = None\n",
    "\n",
    "        return pgd_loss\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self.train_loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.val_loader\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return self.test_loader\n",
    "    \n",
    "#regular kind\n",
    "train_loader = get_dataloader(\"train\", batch_num=2, batch_size=1)\n",
    "val_loader = get_dataloader(\"validation\", batch_num=2, batch_size=1)\n",
    "test_loader = get_dataloader(\"test\", batch_num=2, batch_size=1)\n",
    "\n",
    "\n",
    "# Create the classifier\n",
    "classifier = RobustResnet(train_loader, val_loader, test_loader)\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',  # Monitor validation loss for saving the best model\n",
    "    dirpath='checkpoints/',  # Directory where checkpoints will be saved\n",
    "    filename='best-checkpoint',  # Checkpoint filename\n",
    "    save_top_k=1,  # Save only the best model\n",
    "    mode='min'  # Minimize the monitored metric\n",
    ")\n",
    "\n",
    "# Create a Trainer\n",
    "#TODO.x change this to ensure that the training step is called first instead of the validation step. \n",
    "trainer = Trainer(\n",
    "    max_epochs=3,\n",
    "    accelerator='gpu',  # Specify the accelerator type\n",
    "    devices=1,  # Use 1 GPU if available\n",
    "    callbacks=[checkpoint_callback],  # Add the checkpoint callback\n",
    "    log_every_n_steps=1,  # Log metrics every step\n",
    "    enable_progress_bar=True,  # Enable progress bar for visibility\n",
    "    inference_mode=False  # Added this line to allow gradients during validation\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(classifier)\n",
    "\n"
   ],
   "id": "5b8ccd73ff3dcac6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train ILSVRC/imagenet-1k dataset...\n",
      "Creating dataloader with 2 batches for split train each with size 1\n",
      "Loading validation ILSVRC/imagenet-1k dataset...\n",
      "Creating dataloader with 2 batches for split validation each with size 1\n",
      "Loading test ILSVRC/imagenet-1k dataset...\n",
      "Creating dataloader with 2 batches for split test each with size 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\paras\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:68: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type               | Params | Mode \n",
      "---------------------------------------------------------\n",
      "0 | model     | ResNet             | 25.6 M | train\n",
      "1 | loss_fn   | CrossEntropyLoss   | 0      | train\n",
      "2 | train_acc | MulticlassAccuracy | 0      | train\n",
      "3 | val_acc   | MulticlassAccuracy | 0      | train\n",
      "4 | test_acc  | MulticlassAccuracy | 0      | train\n",
      "---------------------------------------------------------\n",
      "25.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "25.6 M    Total params\n",
      "102.228   Total estimated model params size (MB)\n",
      "155       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "C:\\Users\\paras\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "eeb1b46285a04b289be690fbf6dca29d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paras\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:384: `ModelCheckpoint(monitor='val_loss')` could not find the monitored key in the returned metrics: ['train_ce_loss', 'train_acc', 'epoch', 'step']. HINT: Did you call `log('val_loss', value)` in the `LightningModule`?\n",
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-05T05:27:13.929015Z",
     "start_time": "2024-09-05T05:27:01.451845Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create dataloaders\n",
    "# Robust kind\n",
    "# train_loader = create_dataloader(\"train\", batch_num=2, batch_size=1)\n",
    "# val_loader = create_dataloader(\"validation\", batch_num=2, batch_size=1)\n",
    "# test_loader = create_dataloader(\"test\", batch_num=2, batch_size=1)\n",
    "\n",
    "\n",
    "# Create the classifier\n",
    "classifier = RobustResnet(train_loader, val_loader, test_loader)\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',  # Monitor validation loss for saving the best model\n",
    "    dirpath='checkpoints/',  # Directory where checkpoints will be saved\n",
    "    filename='best-checkpoint',  # Checkpoint filename\n",
    "    save_top_k=1,  # Save only the best model\n",
    "    mode='min'  # Minimize the monitored metric\n",
    ")\n",
    "\n",
    "# Create a Trainer\n",
    "trainer = Trainer(\n",
    "    max_epochs=3,\n",
    "    accelerator='gpu',  # Specify the accelerator type\n",
    "    devices=1,  # Use 1 GPU if available\n",
    "    callbacks=[checkpoint_callback],  # Add the checkpoint callback\n",
    "    log_every_n_steps=1  # Log metrics every step\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(classifier)\n"
   ],
   "id": "49c4e5d6f46f3f06",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\paras\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:68: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type               | Params | Mode \n",
      "---------------------------------------------------------\n",
      "0 | model     | ResNet             | 25.6 M | train\n",
      "1 | loss_fn   | CrossEntropyLoss   | 0      | train\n",
      "2 | train_acc | MulticlassAccuracy | 0      | train\n",
      "3 | val_acc   | MulticlassAccuracy | 0      | train\n",
      "4 | test_acc  | MulticlassAccuracy | 0      | train\n",
      "---------------------------------------------------------\n",
      "25.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "25.6 M    Total params\n",
      "102.228   Total estimated model params size (MB)\n",
      "155       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "C:\\Users\\paras\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n",
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:47\u001B[0m, in \u001B[0;36m_call_and_handle_interrupt\u001B[1;34m(trainer, trainer_fn, *args, **kwargs)\u001B[0m\n\u001B[0;32m     46\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mlauncher\u001B[38;5;241m.\u001B[39mlaunch(trainer_fn, \u001B[38;5;241m*\u001B[39margs, trainer\u001B[38;5;241m=\u001B[39mtrainer, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m---> 47\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m trainer_fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     49\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m _TunerExitException:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:574\u001B[0m, in \u001B[0;36mTrainer._fit_impl\u001B[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[0;32m    568\u001B[0m ckpt_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_checkpoint_connector\u001B[38;5;241m.\u001B[39m_select_ckpt_path(\n\u001B[0;32m    569\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mfn,\n\u001B[0;32m    570\u001B[0m     ckpt_path,\n\u001B[0;32m    571\u001B[0m     model_provided\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m    572\u001B[0m     model_connected\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlightning_module \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    573\u001B[0m )\n\u001B[1;32m--> 574\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mckpt_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mckpt_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    576\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mstopped\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:981\u001B[0m, in \u001B[0;36mTrainer._run\u001B[1;34m(self, model, ckpt_path)\u001B[0m\n\u001B[0;32m    978\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[0;32m    979\u001B[0m \u001B[38;5;66;03m# RUN THE TRAINER\u001B[39;00m\n\u001B[0;32m    980\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[1;32m--> 981\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_stage\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    983\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[0;32m    984\u001B[0m \u001B[38;5;66;03m# POST-Training CLEAN UP\u001B[39;00m\n\u001B[0;32m    985\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1025\u001B[0m, in \u001B[0;36mTrainer._run_stage\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1024\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mset_detect_anomaly(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_detect_anomaly):\n\u001B[1;32m-> 1025\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit_loop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1026\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:197\u001B[0m, in \u001B[0;36m_FitLoop.run\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    196\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrun\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 197\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msetup_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    198\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mskip:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:263\u001B[0m, in \u001B[0;36m_FitLoop.setup_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    262\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_data_fetcher\u001B[38;5;241m.\u001B[39msetup(combined_loader)\n\u001B[1;32m--> 263\u001B[0m \u001B[38;5;28;43miter\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_data_fetcher\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# creates the iterator inside the fetcher\u001B[39;00m\n\u001B[0;32m    264\u001B[0m max_batches \u001B[38;5;241m=\u001B[39m sized_len(combined_loader)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\pytorch_lightning\\loops\\fetchers.py:111\u001B[0m, in \u001B[0;36m_PrefetchDataFetcher.__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    110\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 111\u001B[0m     batch \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__next__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    112\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatches\u001B[38;5;241m.\u001B[39mappend(batch)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\pytorch_lightning\\loops\\fetchers.py:60\u001B[0m, in \u001B[0;36m_DataFetcher.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     59\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 60\u001B[0m     batch \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     61\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\pytorch_lightning\\utilities\\combined_loader.py:341\u001B[0m, in \u001B[0;36mCombinedLoader.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    340\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iterator \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m--> 341\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_iterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    342\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iterator, _Sequential):\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\pytorch_lightning\\utilities\\combined_loader.py:78\u001B[0m, in \u001B[0;36m_MaxSizeCycle.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     77\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 78\u001B[0m     out[i] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43miterators\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     79\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    629\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 630\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    631\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:673\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    672\u001B[0m index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 673\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    674\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:33\u001B[0m, in \u001B[0;36m_IterableDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     32\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 33\u001B[0m     data\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset_iter\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m     34\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\datasets\\iterable_dataset.py:2032\u001B[0m, in \u001B[0;36mIterableDataset.__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   2030\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[1;32m-> 2032\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m key, example \u001B[38;5;129;01min\u001B[39;00m ex_iterable:\n\u001B[0;32m   2033\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfeatures:\n\u001B[0;32m   2034\u001B[0m         \u001B[38;5;66;03m# `IterableDataset` automatically fills missing columns with None.\u001B[39;00m\n\u001B[0;32m   2035\u001B[0m         \u001B[38;5;66;03m# This is done with `_apply_feature_types_on_example`.\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\datasets\\iterable_dataset.py:1520\u001B[0m, in \u001B[0;36mTakeExamplesIterable.__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1519\u001B[0m ex_iterable_num_taken \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_state_dict[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnum_taken\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_state_dict \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m-> 1520\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m key_example \u001B[38;5;129;01min\u001B[39;00m islice(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mex_iterable, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn \u001B[38;5;241m-\u001B[39m ex_iterable_num_taken):\n\u001B[0;32m   1521\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_state_dict:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\datasets\\iterable_dataset.py:1406\u001B[0m, in \u001B[0;36mBufferShuffledExamplesIterable.__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1405\u001B[0m mem_buffer \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m-> 1406\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mex_iterable:\n\u001B[0;32m   1407\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(mem_buffer) \u001B[38;5;241m==\u001B[39m buffer_size:  \u001B[38;5;66;03m# if the buffer is full, pick and example from it\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\datasets\\iterable_dataset.py:954\u001B[0m, in \u001B[0;36mMappedExamplesIterable.__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    953\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 954\u001B[0m     \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iter()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\datasets\\iterable_dataset.py:1034\u001B[0m, in \u001B[0;36mMappedExamplesIterable._iter\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1033\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1034\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m key, example \u001B[38;5;129;01min\u001B[39;00m iterator:\n\u001B[0;32m   1035\u001B[0m         \u001B[38;5;66;03m# If not batched, we can apply the transform and yield the example directly\u001B[39;00m\n\u001B[0;32m   1036\u001B[0m         \u001B[38;5;66;03m# first copy the example, since we might drop some keys\u001B[39;00m\n\u001B[0;32m   1037\u001B[0m         example \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mdict\u001B[39m(example)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\datasets\\iterable_dataset.py:1216\u001B[0m, in \u001B[0;36mFilteredExamplesIterable.__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1215\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1216\u001B[0m     \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iter()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\datasets\\iterable_dataset.py:1272\u001B[0m, in \u001B[0;36mFilteredExamplesIterable._iter\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1271\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1272\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m key, example \u001B[38;5;129;01min\u001B[39;00m iterator:\n\u001B[0;32m   1273\u001B[0m         \u001B[38;5;66;03m# If not batched, we can apply the filtering function direcly\u001B[39;00m\n\u001B[0;32m   1274\u001B[0m         example \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mdict\u001B[39m(example)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\datasets\\iterable_dataset.py:1624\u001B[0m, in \u001B[0;36mTypedExamplesIterable.__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1621\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__iter__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m   1622\u001B[0m     \u001B[38;5;66;03m# Then for each example, `TypedExamplesIterable` automatically fills missing columns with None.\u001B[39;00m\n\u001B[0;32m   1623\u001B[0m     \u001B[38;5;66;03m# This is done with `_apply_feature_types_on_example`.\u001B[39;00m\n\u001B[1;32m-> 1624\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m key, example \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mex_iterable:\n\u001B[0;32m   1625\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m (\n\u001B[0;32m   1626\u001B[0m             key,\n\u001B[0;32m   1627\u001B[0m             _apply_feature_types_on_example(example, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfeatures, token_per_repo_id\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtoken_per_repo_id),\n\u001B[0;32m   1628\u001B[0m         )\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\datasets\\iterable_dataset.py:232\u001B[0m, in \u001B[0;36mShuffledDataSourcesExamplesIterable.__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    231\u001B[0m shard_example_idx_start \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_state_dict[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mshard_example_idx\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_state_dict \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m--> 232\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m key_example \u001B[38;5;129;01min\u001B[39;00m islice(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgenerate_examples_fn(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mgen_kwags), shard_example_idx_start, \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m    233\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_state_dict:\n",
      "File \u001B[1;32m~\\.cache\\huggingface\\modules\\datasets_modules\\datasets\\ILSVRC--imagenet-1k\\07900defe1ccf3404ea7e5e876a64ca41192f6c07406044771544ef1505831e8\\imagenet-1k.py:109\u001B[0m, in \u001B[0;36mImagenet1k._generate_examples\u001B[1;34m(self, archives, split)\u001B[0m\n\u001B[0;32m    108\u001B[0m     label \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m\n\u001B[1;32m--> 109\u001B[0m ex \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mimage\u001B[39m\u001B[38;5;124m\"\u001B[39m: {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpath\u001B[39m\u001B[38;5;124m\"\u001B[39m: path, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbytes\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[43mfile\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m}, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m\"\u001B[39m: label}\n\u001B[0;32m    110\u001B[0m \u001B[38;5;28;01myield\u001B[39;00m idx, ex\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\tarfile.py:685\u001B[0m, in \u001B[0;36m_FileInFile.read\u001B[1;34m(self, size)\u001B[0m\n\u001B[0;32m    684\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfileobj\u001B[38;5;241m.\u001B[39mseek(offset \u001B[38;5;241m+\u001B[39m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mposition \u001B[38;5;241m-\u001B[39m start))\n\u001B[1;32m--> 685\u001B[0m b \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfileobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlength\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    686\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(b) \u001B[38;5;241m!=\u001B[39m length:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\tarfile.py:522\u001B[0m, in \u001B[0;36m_Stream.read\u001B[1;34m(self, size)\u001B[0m\n\u001B[0;32m    521\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m size \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m--> 522\u001B[0m buf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43msize\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    523\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpos \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(buf)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\tarfile.py:540\u001B[0m, in \u001B[0;36m_Stream._read\u001B[1;34m(self, size)\u001B[0m\n\u001B[0;32m    539\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 540\u001B[0m     buf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfileobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbufsize\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    541\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m buf:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\datasets\\utils\\file_utils.py:1117\u001B[0m, in \u001B[0;36m_add_retries_to_file_obj_read_method.<locals>.read_with_retries\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m   1116\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1117\u001B[0m     out \u001B[38;5;241m=\u001B[39m read(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1118\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\huggingface_hub\\hf_file_system.py:765\u001B[0m, in \u001B[0;36mHfFileSystemFile.read\u001B[1;34m(self, length)\u001B[0m\n\u001B[0;32m    764\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m f\u001B[38;5;241m.\u001B[39mread()\n\u001B[1;32m--> 765\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlength\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\fsspec\\spec.py:1941\u001B[0m, in \u001B[0;36mAbstractBufferedFile.read\u001B[1;34m(self, length)\u001B[0m\n\u001B[0;32m   1940\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m-> 1941\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcache\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fetch\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mloc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mloc\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mlength\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1943\u001B[0m logger\u001B[38;5;241m.\u001B[39mdebug(\n\u001B[0;32m   1944\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m read: \u001B[39m\u001B[38;5;132;01m%i\u001B[39;00m\u001B[38;5;124m - \u001B[39m\u001B[38;5;132;01m%i\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   1945\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1948\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcache\u001B[38;5;241m.\u001B[39m_log_stats(),\n\u001B[0;32m   1949\u001B[0m )\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\fsspec\\caching.py:234\u001B[0m, in \u001B[0;36mReadAheadCache._fetch\u001B[1;34m(self, start, end)\u001B[0m\n\u001B[0;32m    233\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtotal_requested_bytes \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m end \u001B[38;5;241m-\u001B[39m start\n\u001B[1;32m--> 234\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcache \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetcher\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstart\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mend\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# new block replaces old\u001B[39;00m\n\u001B[0;32m    235\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstart \u001B[38;5;241m=\u001B[39m start\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\huggingface_hub\\hf_file_system.py:721\u001B[0m, in \u001B[0;36mHfFileSystemFile._fetch_range\u001B[1;34m(self, start, end)\u001B[0m\n\u001B[0;32m    714\u001B[0m url \u001B[38;5;241m=\u001B[39m hf_hub_url(\n\u001B[0;32m    715\u001B[0m     repo_id\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mresolved_path\u001B[38;5;241m.\u001B[39mrepo_id,\n\u001B[0;32m    716\u001B[0m     revision\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mresolved_path\u001B[38;5;241m.\u001B[39mrevision,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    719\u001B[0m     endpoint\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfs\u001B[38;5;241m.\u001B[39mendpoint,\n\u001B[0;32m    720\u001B[0m )\n\u001B[1;32m--> 721\u001B[0m r \u001B[38;5;241m=\u001B[39m \u001B[43mhttp_backoff\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    722\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mGET\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    723\u001B[0m \u001B[43m    \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    724\u001B[0m \u001B[43m    \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    725\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretry_on_status_codes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m502\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m503\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m504\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    726\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mHF_HUB_DOWNLOAD_TIMEOUT\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    727\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    728\u001B[0m hf_raise_for_status(r)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\huggingface_hub\\utils\\_http.py:280\u001B[0m, in \u001B[0;36mhttp_backoff\u001B[1;34m(method, url, max_retries, base_wait_time, max_wait_time, retry_on_exceptions, retry_on_status_codes, **kwargs)\u001B[0m\n\u001B[0;32m    279\u001B[0m \u001B[38;5;66;03m# Perform request and return if status_code is not in the retry list.\u001B[39;00m\n\u001B[1;32m--> 280\u001B[0m response \u001B[38;5;241m=\u001B[39m session\u001B[38;5;241m.\u001B[39mrequest(method\u001B[38;5;241m=\u001B[39mmethod, url\u001B[38;5;241m=\u001B[39murl, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    281\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m response\u001B[38;5;241m.\u001B[39mstatus_code \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m retry_on_status_codes:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\requests\\sessions.py:589\u001B[0m, in \u001B[0;36mSession.request\u001B[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001B[0m\n\u001B[0;32m    588\u001B[0m send_kwargs\u001B[38;5;241m.\u001B[39mupdate(settings)\n\u001B[1;32m--> 589\u001B[0m resp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msend(prep, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39msend_kwargs)\n\u001B[0;32m    591\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m resp\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\requests\\sessions.py:703\u001B[0m, in \u001B[0;36mSession.send\u001B[1;34m(self, request, **kwargs)\u001B[0m\n\u001B[0;32m    702\u001B[0m \u001B[38;5;66;03m# Send the request\u001B[39;00m\n\u001B[1;32m--> 703\u001B[0m r \u001B[38;5;241m=\u001B[39m adapter\u001B[38;5;241m.\u001B[39msend(request, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    705\u001B[0m \u001B[38;5;66;03m# Total elapsed time of the request (approximately)\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\huggingface_hub\\utils\\_http.py:66\u001B[0m, in \u001B[0;36mUniqueRequestIdAdapter.send\u001B[1;34m(self, request, *args, **kwargs)\u001B[0m\n\u001B[0;32m     65\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 66\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39msend(request, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     67\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m requests\u001B[38;5;241m.\u001B[39mRequestException \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\requests\\adapters.py:667\u001B[0m, in \u001B[0;36mHTTPAdapter.send\u001B[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001B[0m\n\u001B[0;32m    666\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 667\u001B[0m     resp \u001B[38;5;241m=\u001B[39m \u001B[43mconn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43murlopen\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    668\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmethod\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    669\u001B[0m \u001B[43m        \u001B[49m\u001B[43murl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    670\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbody\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbody\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    671\u001B[0m \u001B[43m        \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    672\u001B[0m \u001B[43m        \u001B[49m\u001B[43mredirect\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    673\u001B[0m \u001B[43m        \u001B[49m\u001B[43massert_same_host\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    674\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpreload_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    675\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdecode_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    676\u001B[0m \u001B[43m        \u001B[49m\u001B[43mretries\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_retries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    677\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    678\u001B[0m \u001B[43m        \u001B[49m\u001B[43mchunked\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchunked\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    679\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    681\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (ProtocolError, \u001B[38;5;167;01mOSError\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\urllib3\\connectionpool.py:789\u001B[0m, in \u001B[0;36mHTTPConnectionPool.urlopen\u001B[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001B[0m\n\u001B[0;32m    788\u001B[0m \u001B[38;5;66;03m# Make the request on the HTTPConnection object\u001B[39;00m\n\u001B[1;32m--> 789\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_request(\n\u001B[0;32m    790\u001B[0m     conn,\n\u001B[0;32m    791\u001B[0m     method,\n\u001B[0;32m    792\u001B[0m     url,\n\u001B[0;32m    793\u001B[0m     timeout\u001B[38;5;241m=\u001B[39mtimeout_obj,\n\u001B[0;32m    794\u001B[0m     body\u001B[38;5;241m=\u001B[39mbody,\n\u001B[0;32m    795\u001B[0m     headers\u001B[38;5;241m=\u001B[39mheaders,\n\u001B[0;32m    796\u001B[0m     chunked\u001B[38;5;241m=\u001B[39mchunked,\n\u001B[0;32m    797\u001B[0m     retries\u001B[38;5;241m=\u001B[39mretries,\n\u001B[0;32m    798\u001B[0m     response_conn\u001B[38;5;241m=\u001B[39mresponse_conn,\n\u001B[0;32m    799\u001B[0m     preload_content\u001B[38;5;241m=\u001B[39mpreload_content,\n\u001B[0;32m    800\u001B[0m     decode_content\u001B[38;5;241m=\u001B[39mdecode_content,\n\u001B[0;32m    801\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mresponse_kw,\n\u001B[0;32m    802\u001B[0m )\n\u001B[0;32m    804\u001B[0m \u001B[38;5;66;03m# Everything went great!\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\urllib3\\connectionpool.py:536\u001B[0m, in \u001B[0;36mHTTPConnectionPool._make_request\u001B[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001B[0m\n\u001B[0;32m    535\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 536\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43mconn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgetresponse\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    537\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (BaseSSLError, \u001B[38;5;167;01mOSError\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\urllib3\\connection.py:464\u001B[0m, in \u001B[0;36mHTTPConnection.getresponse\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    463\u001B[0m \u001B[38;5;66;03m# Get the response from http.client.HTTPConnection\u001B[39;00m\n\u001B[1;32m--> 464\u001B[0m httplib_response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgetresponse\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    466\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\http\\client.py:1374\u001B[0m, in \u001B[0;36mHTTPConnection.getresponse\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1373\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1374\u001B[0m     \u001B[43mresponse\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbegin\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1375\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mConnectionError\u001B[39;00m:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\http\\client.py:318\u001B[0m, in \u001B[0;36mHTTPResponse.begin\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    317\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m--> 318\u001B[0m     version, status, reason \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_read_status\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    319\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m status \u001B[38;5;241m!=\u001B[39m CONTINUE:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\http\\client.py:279\u001B[0m, in \u001B[0;36mHTTPResponse._read_status\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    278\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_read_status\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m--> 279\u001B[0m     line \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreadline\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_MAXLINE\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124miso-8859-1\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    280\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(line) \u001B[38;5;241m>\u001B[39m _MAXLINE:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\socket.py:705\u001B[0m, in \u001B[0;36mSocketIO.readinto\u001B[1;34m(self, b)\u001B[0m\n\u001B[0;32m    704\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 705\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sock\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrecv_into\u001B[49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    706\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m timeout:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\ssl.py:1274\u001B[0m, in \u001B[0;36mSSLSocket.recv_into\u001B[1;34m(self, buffer, nbytes, flags)\u001B[0m\n\u001B[0;32m   1271\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   1272\u001B[0m           \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m\n\u001B[0;32m   1273\u001B[0m           \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m)\n\u001B[1;32m-> 1274\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnbytes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbuffer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1275\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\ssl.py:1130\u001B[0m, in \u001B[0;36mSSLSocket.read\u001B[1;34m(self, len, buffer)\u001B[0m\n\u001B[0;32m   1129\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m buffer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sslobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbuffer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1131\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[13], line 29\u001B[0m\n\u001B[0;32m     20\u001B[0m trainer \u001B[38;5;241m=\u001B[39m Trainer(\n\u001B[0;32m     21\u001B[0m     max_epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m,\n\u001B[0;32m     22\u001B[0m     accelerator\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgpu\u001B[39m\u001B[38;5;124m'\u001B[39m,  \u001B[38;5;66;03m# Specify the accelerator type\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     25\u001B[0m     log_every_n_steps\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m  \u001B[38;5;66;03m# Log metrics every step\u001B[39;00m\n\u001B[0;32m     26\u001B[0m )\n\u001B[0;32m     28\u001B[0m \u001B[38;5;66;03m# Train the model\u001B[39;00m\n\u001B[1;32m---> 29\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mclassifier\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:538\u001B[0m, in \u001B[0;36mTrainer.fit\u001B[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[0;32m    536\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mstatus \u001B[38;5;241m=\u001B[39m TrainerStatus\u001B[38;5;241m.\u001B[39mRUNNING\n\u001B[0;32m    537\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m--> 538\u001B[0m \u001B[43mcall\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_and_handle_interrupt\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    539\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit_impl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dataloaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_dataloaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdatamodule\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mckpt_path\u001B[49m\n\u001B[0;32m    540\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:64\u001B[0m, in \u001B[0;36m_call_and_handle_interrupt\u001B[1;34m(trainer, trainer_fn, *args, **kwargs)\u001B[0m\n\u001B[0;32m     62\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(launcher, _SubprocessScriptLauncher):\n\u001B[0;32m     63\u001B[0m         launcher\u001B[38;5;241m.\u001B[39mkill(_get_sigkill_signal())\n\u001B[1;32m---> 64\u001B[0m     \u001B[43mexit\u001B[49m(\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m     66\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exception:\n\u001B[0;32m     67\u001B[0m     _interrupt(trainer, exception)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'exit' is not defined"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs/"
   ],
   "id": "be727aefe3b8073a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Test the model\n",
    "trainer.test()"
   ],
   "id": "778377282eab7dde",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "'''\n",
    "TODO:\n",
    "1. check tensorboard here in the cell directly \n",
    "2. evaluate the evaluate code as the checkpointing and reloading of that model is not as straightforward as I thought it would be \n",
    "'''"
   ],
   "id": "dadf7840d0a71ad9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 5. Saving model & Evaluating Results",
   "id": "f2232fa60d92bde6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def load_model_from_checkpoint(checkpoint_path):\n",
    "    # Load the checkpoint\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "\n",
    "    # Create a new instance of your model\n",
    "    finetuned_model = RobustResnet()  # Initialize with the required parameters\n",
    "\n",
    "    # Load the model weights from the checkpoint\n",
    "    finetuned_model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    finetuned_model.eval()\n",
    "    \n",
    "    return finetuned_model\n",
    "\n",
    "def evaluate_model(checkpoint_path, original_model, test_dataloader, device=DEVICE):\n",
    "    # Load the fine-tuned model from the checkpoint\n",
    "    fine_tuned_model = load_model_from_checkpoint(checkpoint_path)\n",
    "\n",
    "    # Move models to the appropriate device\n",
    "    original_model.to(device)\n",
    "    fine_tuned_model.to(device)\n",
    "\n",
    "    # Initialize accuracy metrics\n",
    "    original_acc = Accuracy()\n",
    "    fine_tuned_acc = Accuracy()\n",
    "\n",
    "    # Evaluate the original model\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            images, labels = batch\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Original model predictions\n",
    "            original_logits = original_model(images)\n",
    "            original_acc(original_logits, labels)\n",
    "\n",
    "            # Fine-tuned model predictions\n",
    "            fine_tuned_logits = fine_tuned_model(images)\n",
    "            fine_tuned_acc(fine_tuned_logits, labels)\n",
    "\n",
    "    # Calculate accuracies\n",
    "    original_accuracy = original_acc.compute()\n",
    "    fine_tuned_accuracy = fine_tuned_acc.compute()\n",
    "\n",
    "    print(f'Evaluation Original Model Accuracy: {original_accuracy:.4f}')\n",
    "    print(f'Evaluation Fine-Tuned Model Accuracy: {fine_tuned_accuracy:.4f}')\n",
    "\n",
    "    return original_accuracy, fine_tuned_accuracy"
   ],
   "id": "eed147733535b4b9",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
