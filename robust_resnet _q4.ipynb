{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# TODOs:\n",
    "\n",
    "1. For dataloaders keep the model separate ( or use the same model for train, test and val which just the base one) but for the actual finetuning use a different initialization\n",
    "2. Since filter is possible on dataset, construct a dataset/dataloader with balanced class representation and then perturb it. 3 images * 2 perturbations * 1000 * (1 + 2 augmentations) =18k images ( check if this is enough first of all )\n",
    "3. the model has issues with the validation step. (when is it called, why is grad_fn not available)\n",
    " (check from here -> https://github.com/Lightning-AI/pytorch-lightning/issues/13948)\n"
   ],
   "id": "af1eef554c6a15af"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# About\n",
    "\n",
    "This notebook attempts to finetune a resnet model to be more Robust by leveraging Projected Gradient Descent (PGD) in the two different ways:\n",
    "1. include it as a part of the dataset\n",
    "2. include the projected gradient as well while training along with the regular gradient from the loss function "
   ],
   "id": "9befb1ddab3e4144"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 0. Importing required libraries\n",
   "id": "876d5185378e4464"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-04T02:11:56.673356Z",
     "start_time": "2024-09-04T02:11:56.657340Z"
    }
   },
   "cell_type": "code",
   "source": "# !pip install lightning[extra]",
   "id": "d359d5eb64ade957",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-04T02:12:02.452819Z",
     "start_time": "2024-09-04T02:11:56.676086Z"
    }
   },
   "source": [
    "import os\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "import random\n",
    "from datasets import load_dataset\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from torchmetrics import Accuracy\n",
    "from tqdm import tqdm"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1. Setup",
   "id": "5f6951e549aa8a2c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-04T02:12:04.051126Z",
     "start_time": "2024-09-04T02:12:02.452819Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Manual seed for reproducibility\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Params for PGD\n",
    "ALPHA = 2/255\n",
    "STEPS = 20\n",
    "EPSILON = 8/255\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"device: {DEVICE}\")\n",
    "\n",
    "# model = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "# print(\"initialized the model\")"
   ],
   "id": "8e43067e9bcd293c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1.1 Loading the datasets",
   "id": "547319e9ec62f2d9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-04T02:12:04.066774Z",
     "start_time": "2024-09-04T02:12:04.051126Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess_img(example):\n",
    "    weights = ResNet50_Weights.DEFAULT\n",
    "    preprocess = weights.transforms()\n",
    "    example['image'] = preprocess(example['image'])\n",
    "    return example\n",
    "\n",
    "def get_dataloader(split: str, batch_num: int, batch_size: int):\n",
    "    '''\n",
    "        \n",
    "    :param split: can be either train, test or validation \n",
    "    :return: \n",
    "    '''\n",
    "    print(f\"Loading {split} ILSVRC/imagenet-1k dataset...\")\n",
    "    ds = load_dataset(\"ILSVRC/imagenet-1k\", split=split, streaming=True, trust_remote_code=True)\n",
    "    \n",
    "    # Filter out grayscale images\n",
    "    ds = ds.filter(lambda example: example['image'].mode == 'RGB')\n",
    "    \n",
    "    # Preprocess function will be applied to images on-the-fly whenever they are being accessed in the loop\n",
    "    ds = ds.map(preprocess_img)\n",
    "    ds = ds.shuffle(seed=SEED)\n",
    "    \n",
    "    # Only take desired portion of dataset\n",
    "    ds = ds.take(batch_num * batch_size)\n",
    "    print(f\"Creating dataloader with {batch_num} batches for split {split} each with size {batch_size}\")\n",
    "    return DataLoader(ds, batch_size=batch_size)\n",
    "    "
   ],
   "id": "1e9fafc8606bddd2",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-03T15:28:04.093020Z",
     "start_time": "2024-09-03T15:25:00.487869Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dl = get_dataloader(\"train\", 2, 2)\n",
    "for batch in dl:\n",
    "    print(batch)\n",
    "    break"
   ],
   "id": "5bff44d3b22392f9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train ILSVRC/imagenet-1k dataset...\n",
      "Creating dataloader with 2 batches for split train each with size 2\n",
      "{'image': tensor([[[[-1.7925, -1.7925, -1.7925,  ..., -1.7412, -1.7583, -1.7583],\n",
      "          [-1.7754, -1.7583, -1.7754,  ..., -1.7240, -1.7240, -1.7240],\n",
      "          [-1.7754, -1.7925, -1.7925,  ..., -1.7240, -1.7240, -1.7240],\n",
      "          ...,\n",
      "          [-1.4843, -1.4843, -1.4843,  ..., -1.4843, -1.4843, -1.4843],\n",
      "          [-1.4672, -1.4843, -1.4843,  ..., -1.4843, -1.4672, -1.4672],\n",
      "          [-1.4843, -1.4843, -1.4843,  ..., -1.4843, -1.4843, -1.4672]],\n",
      "\n",
      "         [[-0.7752, -0.7577, -0.7577,  ..., -0.7227, -0.7227, -0.7402],\n",
      "          [-0.7752, -0.7577, -0.7752,  ..., -0.7402, -0.7577, -0.7402],\n",
      "          [-0.7752, -0.7927, -0.7927,  ..., -0.7227, -0.7227, -0.7227],\n",
      "          ...,\n",
      "          [-0.1800, -0.1800, -0.1800,  ..., -0.1625, -0.1800, -0.1800],\n",
      "          [-0.1625, -0.1800, -0.1800,  ..., -0.1800, -0.1625, -0.1625],\n",
      "          [-0.1800, -0.1800, -0.1800,  ..., -0.1800, -0.1800, -0.1975]],\n",
      "\n",
      "         [[ 0.4439,  0.4265,  0.4265,  ...,  0.4962,  0.4962,  0.4962],\n",
      "          [ 0.4439,  0.4265,  0.4265,  ...,  0.5136,  0.5136,  0.5136],\n",
      "          [ 0.4614,  0.4265,  0.4265,  ...,  0.5136,  0.4962,  0.4962],\n",
      "          ...,\n",
      "          [ 1.0365,  1.0365,  1.0365,  ...,  1.0539,  1.0714,  1.0714],\n",
      "          [ 1.0539,  1.0365,  1.0365,  ...,  1.0714,  1.0714,  1.0714],\n",
      "          [ 1.0017,  1.0017,  1.0017,  ...,  1.0539,  1.0365,  1.0365]]],\n",
      "\n",
      "\n",
      "        [[[-0.4911, -0.5253, -0.5938,  ..., -1.6898, -1.6898, -1.6898],\n",
      "          [-0.4739, -0.4568, -0.4226,  ..., -1.7240, -1.6213, -1.4672],\n",
      "          [-0.5253, -0.5424, -0.5253,  ..., -1.5185, -1.4158, -1.3987],\n",
      "          ...,\n",
      "          [-1.3987, -1.3644, -1.3473,  ..., -0.4911, -1.1247, -1.8953],\n",
      "          [-1.4158, -1.3815, -1.3987,  ..., -0.0972,  0.1426, -1.0048],\n",
      "          [-1.4158, -1.4158, -1.4500,  ..., -0.2171, -0.0458,  0.0569]],\n",
      "\n",
      "         [[-0.6527, -0.7227, -0.8102,  ..., -1.6155, -1.6506, -1.6506],\n",
      "          [-0.5476, -0.5301, -0.5301,  ..., -1.6331, -1.5455, -1.4580],\n",
      "          [-0.6176, -0.6176, -0.6176,  ..., -1.4930, -1.4055, -1.4055],\n",
      "          ...,\n",
      "          [-1.5630, -1.5630, -1.5630,  ..., -0.4426, -1.1604, -1.7206],\n",
      "          [-1.5455, -1.5455, -1.5455,  ...,  0.0301,  0.1877, -0.9503],\n",
      "          [-1.5455, -1.5455, -1.5280,  ...,  0.0476,  0.1702,  0.2052]],\n",
      "\n",
      "         [[-0.7936, -0.8284, -0.8458,  ..., -1.5256, -1.5256, -1.6127],\n",
      "          [-0.7238, -0.7413, -0.6715,  ..., -1.5779, -1.5430, -1.4559],\n",
      "          [-0.7064, -0.7413, -0.8807,  ..., -1.4384, -1.3687, -1.3513],\n",
      "          ...,\n",
      "          [-1.5779, -1.6127, -1.6302,  ..., -0.3927, -0.9853, -1.4907],\n",
      "          [-1.5779, -1.5953, -1.5779,  ...,  0.2173,  0.3045, -0.8110],\n",
      "          [-1.6302, -1.5779, -1.5953,  ...,  0.2348,  0.3916,  0.4788]]]]), 'label': tensor([417, 476])}\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-03T15:38:23.708093Z",
     "start_time": "2024-09-03T15:38:04.788423Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Can use this for filtering\n",
    "def chumma(example):\n",
    "    # print(f\"example: {example}\")\n",
    "    # print(f\"example['label']: {example['label']} -> {type(example['label'])}\")\n",
    "    return example['label'] == 916\n",
    "\n",
    "ds = load_dataset(\"ILSVRC/imagenet-1k\", split=\"train\", streaming=True, trust_remote_code=True)\n",
    "ds = ds.filter(chumma)\n",
    "# ds = ds.filter(lambda example: example['image'].mode == 'RGB')\n",
    "# ds = ds.filter(lambda example: example['label'] == 726)\n",
    "ds = ds.map(preprocess_img)\n",
    "ds = ds.take(2 * 2)\n",
    "dl = DataLoader(ds, batch_size=2)\n",
    "\n",
    "for batch in dl:\n",
    "    image, label = batch[\"image\"], batch[\"label\"]\n",
    "    print(label)\n",
    "    break"
   ],
   "id": "7feb09cf8eaaa1b8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([916, 916])\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1.2 My Resnet PGD Attacker",
   "id": "2c84e145464783a6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-04T02:12:04.082790Z",
     "start_time": "2024-09-04T02:12:04.066774Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ResnetPGDAttacker:\n",
    "    def __init__(self, model=None, device=DEVICE):\n",
    "        '''\n",
    "        The PGD attack on Resnet model.\n",
    "        :param model: The resnet model on which we perform the attack\n",
    "        :param dataloader: The dataloader loading the input data on which we perform the attack\n",
    "        '''\n",
    "        self.device = device\n",
    "        \n",
    "        if model == None:\n",
    "            print(f\"Creating new model\")\n",
    "            self.model = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "        else:\n",
    "            print(f\"Using existing model\")\n",
    "            self.model = model     \n",
    "        self.model.to(self.device)\n",
    "        \n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.eps = 0\n",
    "        self.alpha = 0\n",
    "        self.steps = 0\n",
    "        \n",
    "        # Nullify gradient for model params\n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def pgd_attack(self, image, label, eps=None, alpha=None, steps=None):\n",
    "        '''\n",
    "        Create adversarial images for given batch of images and labels\n",
    "\n",
    "        :param image: Batch of input images on which we perform the attack, size (BATCH_SIZE, 3, 224, 224)\n",
    "        :param label: Batch of input labels on which we perform the attack, size (BATCH_SIZE)\n",
    "        :return: Adversarial images for the given input images\n",
    "        '''\n",
    "        if eps is None:\n",
    "            eps = self.eps\n",
    "        if alpha is None:\n",
    "            alpha = self.alpha\n",
    "        if steps is None:\n",
    "            steps = self.steps\n",
    "\n",
    "        images = image.clone().to(self.device)\n",
    "        adv_images = image.clone().to(self.device)\n",
    "        labels = label.clone().to(self.device)\n",
    "\n",
    "        # Starting at a uniformly random point within the eps ball\n",
    "        random_noise = torch.zeros_like(adv_images).uniform_(-eps, eps)\n",
    "        adv_images = adv_images + random_noise\n",
    "        \n",
    "        # Enable gradient tracking for adversarial images\n",
    "        adv_images.requires_grad = True\n",
    "\n",
    "        for _ in range(steps):\n",
    "            # Get model predictions and apply softmax\n",
    "            outputs = self.model(adv_images).softmax(1)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = self.loss_fn(outputs, labels)\n",
    "                        \n",
    "            assert loss.requires_grad == True, f\"loss should be requires_grad {loss}.\"\n",
    "            assert adv_images.requires_grad == True, \"adv_images should be requires_grad\"\n",
    "            \n",
    "            # Compute gradient wrt images\n",
    "            grad = torch.autograd.grad(\n",
    "                loss, adv_images, retain_graph=False, create_graph=False\n",
    "            )[0]\n",
    "\n",
    "            # Gradient update\n",
    "            adv_images = adv_images + alpha * grad.sign()  # Update adversarial images using the sign of the gradient\n",
    "\n",
    "            # Projection step\n",
    "            # Clamping the adversarial images to ensure they are within the L∞ ball of eps radius of original image\n",
    "            adv_images = torch.clamp(adv_images, images - eps, images + eps)\n",
    "\n",
    "        #detaching only in the end\n",
    "        adv_images = adv_images.detach()\n",
    "        return adv_images  # Return the generated adversarial images\n"
   ],
   "id": "29fb7354c84af91b",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 2. Functions related to generating & saving + fetching saved data from a location\n",
    "\n",
    "(TODO) : maybe this is not needed as I am creating a dataset directly!"
   ],
   "id": "4651ce982318861a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-04T02:12:04.098422Z",
     "start_time": "2024-09-04T02:12:04.082790Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def fetch_perturbed_images_from_location(location: str):\n",
    "    \"\"\"\n",
    "    Fetch and concatenate all perturbed images and labels from the specified directory.\n",
    "\n",
    "    :param location: The directory where the .pt files are saved.\n",
    "    :return: Tuple of concatenated adversarial images and labels.\n",
    "    \"\"\"\n",
    "    adv_images = []\n",
    "    labels = []\n",
    "\n",
    "    # Iterate through all .pt files in the specified directory\n",
    "    for file_name in os.listdir(location):\n",
    "        if file_name.endswith(\".pt\"):\n",
    "            file_path = os.path.join(location, file_name)\n",
    "            data = torch.load(file_path)\n",
    "            adv_images.append(data['adv_images'])\n",
    "            labels.append(data['labels'])\n",
    "\n",
    "    # Concatenate all adversarial images and labels\n",
    "    adv_images = torch.cat(adv_images)\n",
    "    labels = torch.cat(labels)\n",
    "\n",
    "    return adv_images, labels\n",
    "\n",
    "def generate_adv_images(dataloader, store: bool = False, store_dir: str = 'adv_images'):\n",
    "    \"\"\"\n",
    "    Generate adversarial images using the PGD method and save them in a directory.\n",
    "\n",
    "    :param dataloader: dataloader of images, labels.\n",
    "    :param model: The ResNet model used for generating adversarial images.\n",
    "    :param store: Boolean flag to indicate whether to save the generated images.\n",
    "    :param store_dir: The directory where the adversarial images and labels will be saved.\n",
    "    :return: Tuple of adversarial images and their corresponding labels.\n",
    "    \"\"\"    \n",
    "    # Initialize the attacker\n",
    "    attacker = ResnetPGDAttacker()\n",
    "\n",
    "    # Generate adversarial images\n",
    "    adv_images = []\n",
    "    labels = []\n",
    "    for batch in tqdm(dataloader):\n",
    "        img_batch, label_batch = batch['image'], batch['label']\n",
    "        adv_images_batch = attacker.pgd_attack(img_batch, label_batch, steps = random.randint(5, STEPS)) #randomly take 5->20 steps\n",
    "        adv_images.append(adv_images_batch)\n",
    "        labels.append(label_batch)\n",
    "\n",
    "    # Concatenate all adversarial images\n",
    "    adv_images = torch.cat(adv_images)\n",
    "    labels = torch.cat(labels)\n",
    "\n",
    "    if store:\n",
    "        # Create the store directory if it doesn't exist\n",
    "        os.makedirs(store_dir, exist_ok=True)\n",
    "    \n",
    "        # Save the adversarial images and labels in multiple files\n",
    "        for j in range(0, len(adv_images), 1000):  # Save in batches of 1000\n",
    "            start = j\n",
    "            end = min(j + 1000, len(adv_images))\n",
    "            file_path = os.path.join(store_dir, f\"adv_images_{start}-{end-1}.pt\")\n",
    "            torch.save({'adv_images': adv_images[start:end], 'labels': labels[start:end]}, file_path)\n",
    "            print(f\"Adversarial images saved at: {file_path}\")\n",
    "        \n",
    "    return store_dir, adv_images, labels"
   ],
   "id": "1e038a305a7bcbe0",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3. Creating the fine-tuning dataset",
   "id": "daebe3fda5c80640"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-04T02:12:05.772049Z",
     "start_time": "2024-09-04T02:12:05.752037Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class RobustDataLoader(DataLoader):\n",
    "    def __init__(self, dataset, split, batch_size=1, batch_num=1, num_perturbations=2, num_transformations=1,\n",
    "                 device=DEVICE,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        Robust DataLoader that includes original, perturbed, and augmented images.\n",
    "\n",
    "        :param dataset: Dataset containing original images and labels.\n",
    "        :param model: The ResNet model used for generating adversarial images.\n",
    "        :param num_perturbations: Number of perturbations to generate for each original image.\n",
    "        :param num_transformations: Number of transformations to apply to each image.\n",
    "        :param kwargs: Additional arguments to be passed to the parent class constructor.\n",
    "        \"\"\"\n",
    "        super().__init__(dataset, batch_size=batch_size,\n",
    "                         **kwargs)  # self.dataset & self.batch_size is defined inside of this       \n",
    "        self.device = device\n",
    "        self.split = split\n",
    "        self.num_perturbations = num_perturbations\n",
    "        self.num_transformations = num_transformations\n",
    "        self.pgd_attacker = ResnetPGDAttacker()\n",
    "        self.batch_num = batch_num\n",
    "        self.dataset_size = self.batch_num * self.batch_size\n",
    "        \n",
    "        if self.split == \"test\": \n",
    "            self.per_yield_limit = self.batch_size\n",
    "            print(f\"setting per_yield_limit to {self.per_yield_limit}, which is the same as batch size as it is in test mode\")\n",
    "        else:\n",
    "            self.per_yield_limit = self.batch_size * (1 + self.num_transformations + self.num_perturbations)\n",
    "            print(f\"setting per_yield_limit to {self.per_yield_limit}, as there are additional #{self.num_transformations} transformations & #{self.num_perturbations} perturbations\")\n",
    "            \n",
    "\n",
    "        # Define transformations\n",
    "        self.transformations = [\n",
    "            transforms.Compose([\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "            ]),\n",
    "            transforms.Compose([\n",
    "                transforms.RandomVerticalFlip(),\n",
    "            ]),\n",
    "            transforms.Compose([\n",
    "                transforms.RandomRotation(20),\n",
    "            ]),\n",
    "            transforms.Compose([\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomRotation(20),\n",
    "                transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "            ]),\n",
    "            transforms.Compose([\n",
    "                transforms.RandomVerticalFlip(),\n",
    "                transforms.RandomRotation(20),\n",
    "                transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "            ])\n",
    "        ]\n",
    "        print(\n",
    "            f\"RobustDataLoader initialized with split={split}, batch_num={batch_num}, batch_size={self.batch_size}, num_perturbations={self.num_perturbations}, num_transformations={self.num_transformations}, per_yield_limit={self.per_yield_limit}\")\n",
    "\n",
    "    def perform_pgd(self, image, label):\n",
    "        random_eps = random.uniform(0.01, 0.3)\n",
    "        random_alpha = random.uniform(0.01, 0.1)\n",
    "        random_steps = random.randint(15, 20)\n",
    "        perturbed_image = self.pgd_attacker.pgd_attack(image.unsqueeze(0), label.unsqueeze(0), eps=random_eps,\n",
    "                                                       alpha=random_alpha, steps=random_steps)\n",
    "        return perturbed_image.squeeze(0).to(self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        '''\n",
    "        For each image, we do T transformations and P perturbations and retain the original image.\n",
    "        \n",
    "        :return: \n",
    "        '''\n",
    "        return (self.batch_num * self.batch_size) * (1 + self.num_transformations + self.num_perturbations)\n",
    "\n",
    "    def __iter__(self):\n",
    "        collected_images = []\n",
    "        collected_labels = []\n",
    "\n",
    "        for datapoint in self.dataset:\n",
    "            image = datapoint['image'].to(self.device)\n",
    "            label = torch.tensor(datapoint['label']).to(self.device)\n",
    "\n",
    "            # Collect original images and labels\n",
    "            collected_images.append(image)\n",
    "            collected_labels.append(label)\n",
    "\n",
    "            if self.split != \"test\":\n",
    "                # Generate and collect perturbations\n",
    "                for _ in range(self.num_perturbations):\n",
    "                    perturbed_image = self.perform_pgd(image, label)\n",
    "                    collected_images.append(perturbed_image)\n",
    "                    collected_labels.append(label)\n",
    "    \n",
    "                # Generate and collect transformations\n",
    "                for _ in range(self.num_transformations):\n",
    "                    transformation_index = random.randint(0, len(self.transformations) - 1)\n",
    "                    transformed_image = self.transformations[transformation_index](image).to(self.device)\n",
    "                    collected_images.append(transformed_image)\n",
    "                    collected_labels.append(label)\n",
    "\n",
    "            # Check if we have collected enough images for the batch\n",
    "            if len(collected_images) == self.per_yield_limit:\n",
    "                # Yield the batch\n",
    "                yield {\n",
    "                    \"image\": torch.stack(collected_images[:self.per_yield_limit]).to(self.device),\n",
    "                    \"label\": torch.tensor(collected_labels[:self.per_yield_limit]).to(self.device)\n",
    "                }\n",
    "\n",
    "                # Clear the lists after yielding\n",
    "                collected_images.clear()\n",
    "                collected_labels.clear()\n"
   ],
   "id": "1e6d83ded5e4d4f0",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-04T02:12:06.333441Z",
     "start_time": "2024-09-04T02:12:06.313440Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_dataloader(split, batch_num=1, batch_size=1, num_perturbations=1, num_transformations=1):\n",
    "    # Load the dataset directly using load_dataset\n",
    "    ds = load_dataset(\"ILSVRC/imagenet-1k\", split=split, streaming=True, trust_remote_code=True)\n",
    "\n",
    "    # Preprocess the dataset\n",
    "    ds = ds.filter(lambda example: example['image'].mode == 'RGB')\n",
    "    ds = ds.map(preprocess_img)\n",
    "    ds = ds.shuffle(seed=SEED)\n",
    "    ds = ds.take(batch_num * batch_size)\n",
    "    print(f\"loaded dataset of {batch_num * batch_size} images & labels from split {split}\")\n",
    "\n",
    "    # Create the RobustDataLoader\n",
    "    data_loader = RobustDataLoader(ds,\n",
    "                                   split=split,\n",
    "                                   num_perturbations=num_perturbations,\n",
    "                                   num_transformations=num_transformations,\n",
    "                                   batch_size=batch_size,\n",
    "                                   batch_num=batch_num\n",
    "                                   )\n",
    "    print(f\"Instantiated dataloader for split {split}, with {len(data_loader)} image, label pairs\")\n",
    "    assert len(data_loader) == batch_num * batch_size * (\n",
    "            1 + num_transformations + num_perturbations), f\"Length of the dataset should have been {batch_num * batch_size * (1 + num_transformations) + num_perturbations}\"\n",
    "    return data_loader\n"
   ],
   "id": "2529e44c8e523535",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.1 Testing Data loaders",
   "id": "8452f74696292c0b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-03T10:30:02.203838Z",
     "start_time": "2024-09-03T10:29:39.111686Z"
    }
   },
   "cell_type": "code",
   "source": [
    "loader = create_dataloader(\"train\", batch_num=2, batch_size=1)\n",
    "\n",
    "for i, batch in enumerate(loader):\n",
    "    print(f\"batch {i}\")\n",
    "    img, label = batch['image'], batch['label']\n",
    "    print(f\"image shape: {img.shape}\")\n",
    "    print(f\"label shape: {label.shape}, labels: {label}\")\n",
    "    print(\"-\"* 30)"
   ],
   "id": "93e92c7aa9b7fffe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded dataset of 2 images & labels from split train\n",
      "Creating new model\n",
      "setting per_yield_limit to 3, as there are additional #1 transformations & #1 perturbations\n",
      "RobustDataLoader initialized with split=train, batch_num=2, batch_size=1, num_perturbations=1, num_transformations=1, per_yield_limit=3\n",
      "Instantiated dataloader for split train, with 6 image, label pairs\n",
      "batch 0\n",
      "image shape: torch.Size([3, 3, 224, 224])\n",
      "label shape: torch.Size([3]), labels: tensor([417, 417, 417], device='cuda:0')\n",
      "------------------------------\n",
      "batch 1\n",
      "image shape: torch.Size([3, 3, 224, 224])\n",
      "label shape: torch.Size([3]), labels: tensor([476, 476, 476], device='cuda:0')\n",
      "------------------------------\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-03T10:30:23.117198Z",
     "start_time": "2024-09-03T10:30:02.203838Z"
    }
   },
   "cell_type": "code",
   "source": [
    "loader = create_dataloader(\"validation\", batch_num=2, batch_size=1)\n",
    "for batch in loader:\n",
    "    print(f\"val batch {i}\")\n",
    "    img, label = batch['image'], batch['label']\n",
    "    print(f\"image shape: {img.shape}\")\n",
    "    print(f\"label shape: {label.shape}\")\n",
    "    print(\"-\"* 30)\n"
   ],
   "id": "4ee1538f4d827db2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded dataset of 2 images & labels from split validation\n",
      "Creating new model\n",
      "setting per_yield_limit to 3, as there are additional #1 transformations & #1 perturbations\n",
      "RobustDataLoader initialized with split=validation, batch_num=2, batch_size=1, num_perturbations=1, num_transformations=1, per_yield_limit=3\n",
      "Instantiated dataloader for split validation, with 6 image, label pairs\n",
      "val batch 1\n",
      "image shape: torch.Size([3, 3, 224, 224])\n",
      "label shape: torch.Size([3])\n",
      "------------------------------\n",
      "val batch 1\n",
      "image shape: torch.Size([3, 3, 224, 224])\n",
      "label shape: torch.Size([3])\n",
      "------------------------------\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-03T10:30:43.630923Z",
     "start_time": "2024-09-03T10:30:23.117198Z"
    }
   },
   "cell_type": "code",
   "source": [
    "loader = create_dataloader(\"test\", batch_num=2, batch_size=1)\n",
    "for batch in loader:\n",
    "    print(f\"test batch {i}\")\n",
    "    img, label = batch['image'], batch['label']\n",
    "    print(f\"image shape: {img.shape}\")\n",
    "    print(f\"label shape: {label.shape}\")\n",
    "    print(\"-\"* 30)\n",
    "\n",
    "del loader"
   ],
   "id": "2b16508d38b65829",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded dataset of 2 images & labels from split test\n",
      "Creating new model\n",
      "setting per_yield_limit to 1, which is the same as batch size as it is in test mode\n",
      "RobustDataLoader initialized with split=test, batch_num=2, batch_size=1, num_perturbations=1, num_transformations=1, per_yield_limit=1\n",
      "Instantiated dataloader for split test, with 6 image, label pairs\n",
      "test batch 1\n",
      "image shape: torch.Size([1, 3, 224, 224])\n",
      "label shape: torch.Size([1])\n",
      "------------------------------\n",
      "test batch 1\n",
      "image shape: torch.Size([1, 3, 224, 224])\n",
      "label shape: torch.Size([1])\n",
      "------------------------------\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 4. Implementing Robust Resnet",
   "id": "f2b608c00b6103c6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-04T02:27:56.150889Z",
     "start_time": "2024-09-04T02:26:33.070581Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class RobustResnet(LightningModule):\n",
    "    def __init__(self, train_loader, val_loader, test_loader, learning_rate=1e-3):\n",
    "        super().__init__()\n",
    "        self.model = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.train_loader = train_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.learning_rate = learning_rate\n",
    "        self.train_acc = Accuracy(\"multiclass\", num_classes=1000)\n",
    "        self.val_acc = Accuracy(\"multiclass\", num_classes=1000)\n",
    "        self.test_acc = Accuracy(\"multiclass\", num_classes=1000)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, labels = batch['image'], batch['label']\n",
    "        logits = self.model(images)\n",
    "        ce_loss = self.loss_fn(logits, labels)\n",
    "        pgd_loss = self.pgd_loss(images, labels)\n",
    "        loss = ce_loss + pgd_loss\n",
    "\n",
    "        self.train_acc(logits, labels)\n",
    "        self.log('train_ce_loss', ce_loss)\n",
    "        self.log('train_pgd_loss', pgd_loss)\n",
    "        self.log('train_loss', loss)\n",
    "        self.log('train_acc', self.train_acc, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images, labels = batch['image'], batch['label']\n",
    "        logits = self.model(images)\n",
    "        ce_loss = self.loss_fn(logits, labels)\n",
    "        pgd_loss = self.pgd_loss(images, labels)\n",
    "        loss = ce_loss + pgd_loss\n",
    "        self.val_acc(logits, labels)\n",
    "        self.log('val_ce_loss', ce_loss, on_step=False, on_epoch=True)\n",
    "        self.log('val_pgd_loss', pgd_loss, on_step=False, on_epoch=True)\n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True)\n",
    "        self.log('val_acc', self.val_acc, on_step=False, on_epoch=True)\n",
    "\n",
    "    #TODO. this wont work as the labels will be -1 in this case\n",
    "    # def test_step(self, batch, batch_idx):\n",
    "    #     images, labels = batch\n",
    "    #     logits = self.model(images)\n",
    "    #     ce_loss = F.cross_entropy(logits, labels)\n",
    "    #     pgd_loss = self.pgd_loss(images, labels)\n",
    "    #     loss = ce_loss + pgd_loss\n",
    "    #     acc = self.test_acc(logits, labels)\n",
    "    #     self.log('test_ce_loss', ce_loss)\n",
    "    #     self.log('test_pgd_loss', pgd_loss)\n",
    "    #     self.log('test_loss', loss)\n",
    "    #     self.log('test_acc', acc)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    def pgd_loss(self, images, labels):\n",
    "        # randomize pgd params\n",
    "        eps = random.uniform(0.01, 0.3)\n",
    "        alpha = random.uniform(0.01, 0.1)\n",
    "        steps = random.randint(15, 20)\n",
    "        adv_images = images.clone() #TODO.maybe this is the problem?\n",
    "        #TODO.x print this and see if it has a grad_fn function at all \n",
    "        \n",
    "        # Starting at a uniformly random point within the eps ball\n",
    "        random_noise = torch.zeros_like(adv_images).uniform_(-eps, eps)\n",
    "        adv_images = adv_images + random_noise\n",
    "        \n",
    "        # Enable gradient tracking for adversarial images\n",
    "        adv_images.requires_grad = True\n",
    "        \n",
    "        # do randomized PGD process and compute loss\n",
    "        for _ in range(steps):\n",
    "            # Get model predictions and apply softmax\n",
    "            outputs = self.model(adv_images).softmax(1)\n",
    "\n",
    "            # Calculate loss\n",
    "            pgd_loss = self.loss_fn(outputs, labels)\n",
    "\n",
    "            # Compute gradient wrt images\n",
    "            grad = torch.autograd.grad(\n",
    "                pgd_loss, adv_images, retain_graph=True, create_graph=True\n",
    "            )\n",
    "            grad = grad[0]\n",
    "\n",
    "            # Gradient update\n",
    "            adv_images = adv_images + alpha * grad.sign()  # Update adversarial images using the sign of the gradient\n",
    "\n",
    "            # Projection step\n",
    "            # Clamping the adversarial images to ensure they are within the L∞ ball of eps radius of original image\n",
    "            adv_images = torch.clamp(adv_images, images - eps, images + eps)\n",
    "\n",
    "        return pgd_loss\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self.train_loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.val_loader\n",
    "\n",
    "    # def test_dataloader(self):\n",
    "    #     return self.test_loader\n",
    "    \n",
    "#regular kind\n",
    "train_loader = get_dataloader(\"train\", batch_num=2, batch_size=1)\n",
    "val_loader = get_dataloader(\"validation\", batch_num=2, batch_size=1)\n",
    "test_loader = get_dataloader(\"test\", batch_num=2, batch_size=1)\n",
    "\n",
    "\n",
    "# Create the classifier\n",
    "classifier = RobustResnet(train_loader, val_loader, test_loader)\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',  # Monitor validation loss for saving the best model\n",
    "    dirpath='checkpoints/',  # Directory where checkpoints will be saved\n",
    "    filename='best-checkpoint',  # Checkpoint filename\n",
    "    save_top_k=1,  # Save only the best model\n",
    "    mode='min'  # Minimize the monitored metric\n",
    ")\n",
    "\n",
    "# Create a Trainer\n",
    "#TODO.x check fi training step can be done first insteadf of validation step\n",
    "trainer = Trainer(\n",
    "    max_epochs=3,\n",
    "    accelerator='gpu',  # Specify the accelerator type\n",
    "    devices=1,  # Use 1 GPU if available\n",
    "    callbacks=[checkpoint_callback],  # Add the checkpoint callback\n",
    "    log_every_n_steps=1  # Log metrics every step\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(classifier)\n",
    "\n"
   ],
   "id": "5b8ccd73ff3dcac6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train ILSVRC/imagenet-1k dataset...\n",
      "Creating dataloader with 2 batches for split train each with size 1\n",
      "Loading validation ILSVRC/imagenet-1k dataset...\n",
      "Creating dataloader with 2 batches for split validation each with size 1\n",
      "Loading test ILSVRC/imagenet-1k dataset...\n",
      "Creating dataloader with 2 batches for split test each with size 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type               | Params | Mode \n",
      "---------------------------------------------------------\n",
      "0 | model     | ResNet             | 25.6 M | train\n",
      "1 | loss_fn   | CrossEntropyLoss   | 0      | train\n",
      "2 | train_acc | MulticlassAccuracy | 0      | train\n",
      "3 | val_acc   | MulticlassAccuracy | 0      | train\n",
      "4 | test_acc  | MulticlassAccuracy | 0      | train\n",
      "---------------------------------------------------------\n",
      "25.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "25.6 M    Total params\n",
      "102.228   Total estimated model params size (MB)\n",
      "155       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f464a74921bb4af2ac4d0ab3b0d317c5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paras\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[13], line 132\u001B[0m\n\u001B[0;32m    123\u001B[0m trainer \u001B[38;5;241m=\u001B[39m Trainer(\n\u001B[0;32m    124\u001B[0m     max_epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m,\n\u001B[0;32m    125\u001B[0m     accelerator\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgpu\u001B[39m\u001B[38;5;124m'\u001B[39m,  \u001B[38;5;66;03m# Specify the accelerator type\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    128\u001B[0m     log_every_n_steps\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m  \u001B[38;5;66;03m# Log metrics every step\u001B[39;00m\n\u001B[0;32m    129\u001B[0m )\n\u001B[0;32m    131\u001B[0m \u001B[38;5;66;03m# Train the model\u001B[39;00m\n\u001B[1;32m--> 132\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mclassifier\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:538\u001B[0m, in \u001B[0;36mTrainer.fit\u001B[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[0;32m    536\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mstatus \u001B[38;5;241m=\u001B[39m TrainerStatus\u001B[38;5;241m.\u001B[39mRUNNING\n\u001B[0;32m    537\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m--> 538\u001B[0m \u001B[43mcall\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_and_handle_interrupt\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    539\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit_impl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dataloaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_dataloaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdatamodule\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mckpt_path\u001B[49m\n\u001B[0;32m    540\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:47\u001B[0m, in \u001B[0;36m_call_and_handle_interrupt\u001B[1;34m(trainer, trainer_fn, *args, **kwargs)\u001B[0m\n\u001B[0;32m     45\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mlauncher \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m     46\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mlauncher\u001B[38;5;241m.\u001B[39mlaunch(trainer_fn, \u001B[38;5;241m*\u001B[39margs, trainer\u001B[38;5;241m=\u001B[39mtrainer, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m---> 47\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m trainer_fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     49\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m _TunerExitException:\n\u001B[0;32m     50\u001B[0m     _call_teardown_hook(trainer)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:574\u001B[0m, in \u001B[0;36mTrainer._fit_impl\u001B[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[0;32m    567\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mfn \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    568\u001B[0m ckpt_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_checkpoint_connector\u001B[38;5;241m.\u001B[39m_select_ckpt_path(\n\u001B[0;32m    569\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mfn,\n\u001B[0;32m    570\u001B[0m     ckpt_path,\n\u001B[0;32m    571\u001B[0m     model_provided\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m    572\u001B[0m     model_connected\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlightning_module \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    573\u001B[0m )\n\u001B[1;32m--> 574\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mckpt_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mckpt_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    576\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mstopped\n\u001B[0;32m    577\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:981\u001B[0m, in \u001B[0;36mTrainer._run\u001B[1;34m(self, model, ckpt_path)\u001B[0m\n\u001B[0;32m    976\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_signal_connector\u001B[38;5;241m.\u001B[39mregister_signal_handlers()\n\u001B[0;32m    978\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[0;32m    979\u001B[0m \u001B[38;5;66;03m# RUN THE TRAINER\u001B[39;00m\n\u001B[0;32m    980\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[1;32m--> 981\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_stage\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    983\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[0;32m    984\u001B[0m \u001B[38;5;66;03m# POST-Training CLEAN UP\u001B[39;00m\n\u001B[0;32m    985\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[0;32m    986\u001B[0m log\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: trainer tearing down\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1023\u001B[0m, in \u001B[0;36mTrainer._run_stage\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1021\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining:\n\u001B[0;32m   1022\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m isolate_rng():\n\u001B[1;32m-> 1023\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_sanity_check\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1024\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mset_detect_anomaly(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_detect_anomaly):\n\u001B[0;32m   1025\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfit_loop\u001B[38;5;241m.\u001B[39mrun()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1052\u001B[0m, in \u001B[0;36mTrainer._run_sanity_check\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1049\u001B[0m call\u001B[38;5;241m.\u001B[39m_call_callback_hooks(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mon_sanity_check_start\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   1051\u001B[0m \u001B[38;5;66;03m# run eval step\u001B[39;00m\n\u001B[1;32m-> 1052\u001B[0m \u001B[43mval_loop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1054\u001B[0m call\u001B[38;5;241m.\u001B[39m_call_callback_hooks(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mon_sanity_check_end\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   1056\u001B[0m \u001B[38;5;66;03m# reset logger connector\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\pytorch_lightning\\loops\\utilities.py:178\u001B[0m, in \u001B[0;36m_no_grad_context.<locals>._decorator\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    176\u001B[0m     context_manager \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mno_grad\n\u001B[0;32m    177\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m context_manager():\n\u001B[1;32m--> 178\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m loop_run(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\pytorch_lightning\\loops\\evaluation_loop.py:135\u001B[0m, in \u001B[0;36m_EvaluationLoop.run\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    133\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_progress\u001B[38;5;241m.\u001B[39mis_last_batch \u001B[38;5;241m=\u001B[39m data_fetcher\u001B[38;5;241m.\u001B[39mdone\n\u001B[0;32m    134\u001B[0m     \u001B[38;5;66;03m# run step hooks\u001B[39;00m\n\u001B[1;32m--> 135\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_evaluation_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdataloader_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdataloader_iter\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    136\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m:\n\u001B[0;32m    137\u001B[0m     \u001B[38;5;66;03m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001B[39;00m\n\u001B[0;32m    138\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\pytorch_lightning\\loops\\evaluation_loop.py:396\u001B[0m, in \u001B[0;36m_EvaluationLoop._evaluation_step\u001B[1;34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001B[0m\n\u001B[0;32m    390\u001B[0m hook_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest_step\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mtesting \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvalidation_step\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    391\u001B[0m step_args \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m    392\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_step_args_from_hook_kwargs(hook_kwargs, hook_name)\n\u001B[0;32m    393\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m using_dataloader_iter\n\u001B[0;32m    394\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m (dataloader_iter,)\n\u001B[0;32m    395\u001B[0m )\n\u001B[1;32m--> 396\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[43mcall\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_strategy_hook\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrainer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhook_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mstep_args\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    398\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_progress\u001B[38;5;241m.\u001B[39mincrement_processed()\n\u001B[0;32m    400\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m using_dataloader_iter:\n\u001B[0;32m    401\u001B[0m     \u001B[38;5;66;03m# update the hook kwargs now that the step method might have consumed the iterator\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:319\u001B[0m, in \u001B[0;36m_call_strategy_hook\u001B[1;34m(trainer, hook_name, *args, **kwargs)\u001B[0m\n\u001B[0;32m    316\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    318\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mprofile(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[Strategy]\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mhook_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m--> 319\u001B[0m     output \u001B[38;5;241m=\u001B[39m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    321\u001B[0m \u001B[38;5;66;03m# restore current_fx when nested context\u001B[39;00m\n\u001B[0;32m    322\u001B[0m pl_module\u001B[38;5;241m.\u001B[39m_current_fx_name \u001B[38;5;241m=\u001B[39m prev_fx_name\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:411\u001B[0m, in \u001B[0;36mStrategy.validation_step\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    409\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlightning_module:\n\u001B[0;32m    410\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_redirection(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlightning_module, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvalidation_step\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m--> 411\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlightning_module\u001B[38;5;241m.\u001B[39mvalidation_step(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "Cell \u001B[1;32mIn[13], line 35\u001B[0m, in \u001B[0;36mRobustResnet.validation_step\u001B[1;34m(self, batch, batch_idx)\u001B[0m\n\u001B[0;32m     33\u001B[0m logits \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel(images)\n\u001B[0;32m     34\u001B[0m ce_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloss_fn(logits, labels)\n\u001B[1;32m---> 35\u001B[0m pgd_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpgd_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     36\u001B[0m loss \u001B[38;5;241m=\u001B[39m ce_loss \u001B[38;5;241m+\u001B[39m pgd_loss\n\u001B[0;32m     37\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mval_acc(logits, labels)\n",
      "Cell \u001B[1;32mIn[13], line 82\u001B[0m, in \u001B[0;36mRobustResnet.pgd_loss\u001B[1;34m(self, images, labels)\u001B[0m\n\u001B[0;32m     79\u001B[0m pgd_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloss_fn(outputs, labels)\n\u001B[0;32m     81\u001B[0m \u001B[38;5;66;03m# Compute gradient wrt images\u001B[39;00m\n\u001B[1;32m---> 82\u001B[0m grad \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgrad\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     83\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpgd_loss\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43madv_images\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\n\u001B[0;32m     84\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     85\u001B[0m grad \u001B[38;5;241m=\u001B[39m grad[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m     87\u001B[0m \u001B[38;5;66;03m# Gradient update\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\torch\\autograd\\__init__.py:436\u001B[0m, in \u001B[0;36mgrad\u001B[1;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001B[0m\n\u001B[0;32m    432\u001B[0m     result \u001B[38;5;241m=\u001B[39m _vmap_internals\u001B[38;5;241m.\u001B[39m_vmap(vjp, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m0\u001B[39m, allow_none_pass_through\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)(\n\u001B[0;32m    433\u001B[0m         grad_outputs_\n\u001B[0;32m    434\u001B[0m     )\n\u001B[0;32m    435\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 436\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    437\u001B[0m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    438\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgrad_outputs_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    439\u001B[0m \u001B[43m        \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    440\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    441\u001B[0m \u001B[43m        \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    442\u001B[0m \u001B[43m        \u001B[49m\u001B[43mallow_unused\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    443\u001B[0m \u001B[43m        \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    444\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    445\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m materialize_grads:\n\u001B[0;32m    446\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28many\u001B[39m(\n\u001B[0;32m    447\u001B[0m         result[i] \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_tensor_like(inputs[i])\n\u001B[0;32m    448\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(inputs))\n\u001B[0;32m    449\u001B[0m     ):\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\torch\\autograd\\graph.py:768\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[1;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[0;32m    766\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[0;32m    767\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 768\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m Variable\u001B[38;5;241m.\u001B[39m_execution_engine\u001B[38;5;241m.\u001B[39mrun_backward(  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[0;32m    769\u001B[0m         t_outputs, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[0;32m    770\u001B[0m     )  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[0;32m    771\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    772\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[1;31mRuntimeError\u001B[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-04T02:12:58.190254Z",
     "start_time": "2024-09-04T02:12:14.817690Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create dataloaders\n",
    "# Robust kind\n",
    "# train_loader = create_dataloader(\"train\", batch_num=2, batch_size=1)\n",
    "# val_loader = create_dataloader(\"validation\", batch_num=2, batch_size=1)\n",
    "# test_loader = create_dataloader(\"test\", batch_num=2, batch_size=1)\n",
    "\n",
    "\n",
    "# Create the classifier\n",
    "classifier = RobustResnet(train_loader, val_loader, test_loader)\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',  # Monitor validation loss for saving the best model\n",
    "    dirpath='checkpoints/',  # Directory where checkpoints will be saved\n",
    "    filename='best-checkpoint',  # Checkpoint filename\n",
    "    save_top_k=1,  # Save only the best model\n",
    "    mode='min'  # Minimize the monitored metric\n",
    ")\n",
    "\n",
    "# Create a Trainer\n",
    "trainer = Trainer(\n",
    "    max_epochs=3,\n",
    "    accelerator='gpu',  # Specify the accelerator type\n",
    "    devices=1,  # Use 1 GPU if available\n",
    "    callbacks=[checkpoint_callback],  # Add the checkpoint callback\n",
    "    log_every_n_steps=1  # Log metrics every step\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(classifier)\n"
   ],
   "id": "49c4e5d6f46f3f06",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train ILSVRC/imagenet-1k dataset...\n",
      "Creating dataloader with 2 batches for split train each with size 1\n",
      "Loading validation ILSVRC/imagenet-1k dataset...\n",
      "Creating dataloader with 2 batches for split validation each with size 1\n",
      "Loading test ILSVRC/imagenet-1k dataset...\n",
      "Creating dataloader with 2 batches for split test each with size 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type               | Params | Mode \n",
      "---------------------------------------------------------\n",
      "0 | model     | ResNet             | 25.6 M | train\n",
      "1 | loss_fn   | CrossEntropyLoss   | 0      | train\n",
      "2 | train_acc | MulticlassAccuracy | 0      | train\n",
      "3 | val_acc   | MulticlassAccuracy | 0      | train\n",
      "4 | test_acc  | MulticlassAccuracy | 0      | train\n",
      "---------------------------------------------------------\n",
      "25.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "25.6 M    Total params\n",
      "102.228   Total estimated model params size (MB)\n",
      "155       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c648abd87c2948158afdc0c3f1ea9b2f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paras\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "conv2d() received an invalid combination of arguments - got (str, Parameter, NoneType, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias = None, tuple of ints stride = 1, tuple of ints padding = 0, tuple of ints dilation = 1, int groups = 1)\n      didn't match because some of the arguments have invalid types: (!str!, !Parameter!, !NoneType!, !tuple of (int, int)!, !tuple of (int, int)!, !tuple of (int, int)!, !int!)\n * (Tensor input, Tensor weight, Tensor bias = None, tuple of ints stride = 1, str padding = \"valid\", tuple of ints dilation = 1, int groups = 1)\n      didn't match because some of the arguments have invalid types: (!str!, !Parameter!, !NoneType!, !tuple of (int, int)!, !tuple of (int, int)!, !tuple of (int, int)!, !int!)\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[10], line 35\u001B[0m\n\u001B[0;32m     26\u001B[0m trainer \u001B[38;5;241m=\u001B[39m Trainer(\n\u001B[0;32m     27\u001B[0m     max_epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m,\n\u001B[0;32m     28\u001B[0m     accelerator\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgpu\u001B[39m\u001B[38;5;124m'\u001B[39m,  \u001B[38;5;66;03m# Specify the accelerator type\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     31\u001B[0m     log_every_n_steps\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m  \u001B[38;5;66;03m# Log metrics every step\u001B[39;00m\n\u001B[0;32m     32\u001B[0m )\n\u001B[0;32m     34\u001B[0m \u001B[38;5;66;03m# Train the model\u001B[39;00m\n\u001B[1;32m---> 35\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mclassifier\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:538\u001B[0m, in \u001B[0;36mTrainer.fit\u001B[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[0;32m    536\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mstatus \u001B[38;5;241m=\u001B[39m TrainerStatus\u001B[38;5;241m.\u001B[39mRUNNING\n\u001B[0;32m    537\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m--> 538\u001B[0m \u001B[43mcall\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_and_handle_interrupt\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    539\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit_impl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dataloaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_dataloaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdatamodule\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mckpt_path\u001B[49m\n\u001B[0;32m    540\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:47\u001B[0m, in \u001B[0;36m_call_and_handle_interrupt\u001B[1;34m(trainer, trainer_fn, *args, **kwargs)\u001B[0m\n\u001B[0;32m     45\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mlauncher \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m     46\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mlauncher\u001B[38;5;241m.\u001B[39mlaunch(trainer_fn, \u001B[38;5;241m*\u001B[39margs, trainer\u001B[38;5;241m=\u001B[39mtrainer, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m---> 47\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m trainer_fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     49\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m _TunerExitException:\n\u001B[0;32m     50\u001B[0m     _call_teardown_hook(trainer)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:574\u001B[0m, in \u001B[0;36mTrainer._fit_impl\u001B[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[0;32m    567\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mfn \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    568\u001B[0m ckpt_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_checkpoint_connector\u001B[38;5;241m.\u001B[39m_select_ckpt_path(\n\u001B[0;32m    569\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mfn,\n\u001B[0;32m    570\u001B[0m     ckpt_path,\n\u001B[0;32m    571\u001B[0m     model_provided\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m    572\u001B[0m     model_connected\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlightning_module \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    573\u001B[0m )\n\u001B[1;32m--> 574\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mckpt_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mckpt_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    576\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mstopped\n\u001B[0;32m    577\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:981\u001B[0m, in \u001B[0;36mTrainer._run\u001B[1;34m(self, model, ckpt_path)\u001B[0m\n\u001B[0;32m    976\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_signal_connector\u001B[38;5;241m.\u001B[39mregister_signal_handlers()\n\u001B[0;32m    978\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[0;32m    979\u001B[0m \u001B[38;5;66;03m# RUN THE TRAINER\u001B[39;00m\n\u001B[0;32m    980\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[1;32m--> 981\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_stage\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    983\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[0;32m    984\u001B[0m \u001B[38;5;66;03m# POST-Training CLEAN UP\u001B[39;00m\n\u001B[0;32m    985\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[0;32m    986\u001B[0m log\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: trainer tearing down\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1023\u001B[0m, in \u001B[0;36mTrainer._run_stage\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1021\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining:\n\u001B[0;32m   1022\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m isolate_rng():\n\u001B[1;32m-> 1023\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_sanity_check\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1024\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mset_detect_anomaly(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_detect_anomaly):\n\u001B[0;32m   1025\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfit_loop\u001B[38;5;241m.\u001B[39mrun()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1052\u001B[0m, in \u001B[0;36mTrainer._run_sanity_check\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1049\u001B[0m call\u001B[38;5;241m.\u001B[39m_call_callback_hooks(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mon_sanity_check_start\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   1051\u001B[0m \u001B[38;5;66;03m# run eval step\u001B[39;00m\n\u001B[1;32m-> 1052\u001B[0m \u001B[43mval_loop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1054\u001B[0m call\u001B[38;5;241m.\u001B[39m_call_callback_hooks(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mon_sanity_check_end\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   1056\u001B[0m \u001B[38;5;66;03m# reset logger connector\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\pytorch_lightning\\loops\\utilities.py:178\u001B[0m, in \u001B[0;36m_no_grad_context.<locals>._decorator\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    176\u001B[0m     context_manager \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mno_grad\n\u001B[0;32m    177\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m context_manager():\n\u001B[1;32m--> 178\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m loop_run(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\pytorch_lightning\\loops\\evaluation_loop.py:135\u001B[0m, in \u001B[0;36m_EvaluationLoop.run\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    133\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_progress\u001B[38;5;241m.\u001B[39mis_last_batch \u001B[38;5;241m=\u001B[39m data_fetcher\u001B[38;5;241m.\u001B[39mdone\n\u001B[0;32m    134\u001B[0m     \u001B[38;5;66;03m# run step hooks\u001B[39;00m\n\u001B[1;32m--> 135\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_evaluation_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdataloader_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdataloader_iter\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    136\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m:\n\u001B[0;32m    137\u001B[0m     \u001B[38;5;66;03m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001B[39;00m\n\u001B[0;32m    138\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\pytorch_lightning\\loops\\evaluation_loop.py:396\u001B[0m, in \u001B[0;36m_EvaluationLoop._evaluation_step\u001B[1;34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001B[0m\n\u001B[0;32m    390\u001B[0m hook_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest_step\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mtesting \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvalidation_step\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    391\u001B[0m step_args \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m    392\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_step_args_from_hook_kwargs(hook_kwargs, hook_name)\n\u001B[0;32m    393\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m using_dataloader_iter\n\u001B[0;32m    394\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m (dataloader_iter,)\n\u001B[0;32m    395\u001B[0m )\n\u001B[1;32m--> 396\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[43mcall\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_strategy_hook\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrainer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhook_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mstep_args\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    398\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_progress\u001B[38;5;241m.\u001B[39mincrement_processed()\n\u001B[0;32m    400\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m using_dataloader_iter:\n\u001B[0;32m    401\u001B[0m     \u001B[38;5;66;03m# update the hook kwargs now that the step method might have consumed the iterator\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:319\u001B[0m, in \u001B[0;36m_call_strategy_hook\u001B[1;34m(trainer, hook_name, *args, **kwargs)\u001B[0m\n\u001B[0;32m    316\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    318\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mprofile(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[Strategy]\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mhook_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m--> 319\u001B[0m     output \u001B[38;5;241m=\u001B[39m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    321\u001B[0m \u001B[38;5;66;03m# restore current_fx when nested context\u001B[39;00m\n\u001B[0;32m    322\u001B[0m pl_module\u001B[38;5;241m.\u001B[39m_current_fx_name \u001B[38;5;241m=\u001B[39m prev_fx_name\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:411\u001B[0m, in \u001B[0;36mStrategy.validation_step\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    409\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlightning_module:\n\u001B[0;32m    410\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_redirection(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlightning_module, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvalidation_step\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m--> 411\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlightning_module\u001B[38;5;241m.\u001B[39mvalidation_step(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "Cell \u001B[1;32mIn[9], line 33\u001B[0m, in \u001B[0;36mRobustResnet.validation_step\u001B[1;34m(self, batch, batch_idx)\u001B[0m\n\u001B[0;32m     31\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mvalidation_step\u001B[39m(\u001B[38;5;28mself\u001B[39m, batch, batch_idx):\n\u001B[0;32m     32\u001B[0m     images, labels \u001B[38;5;241m=\u001B[39m batch\n\u001B[1;32m---> 33\u001B[0m     logits \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimages\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     34\u001B[0m     ce_loss \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mcross_entropy(logits, labels)\n\u001B[0;32m     35\u001B[0m     pgd_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpgd_loss(images, labels)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\torchvision\\models\\resnet.py:285\u001B[0m, in \u001B[0;36mResNet.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m    284\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 285\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_forward_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\torchvision\\models\\resnet.py:268\u001B[0m, in \u001B[0;36mResNet._forward_impl\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m    266\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_forward_impl\u001B[39m(\u001B[38;5;28mself\u001B[39m, x: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m    267\u001B[0m     \u001B[38;5;66;03m# See note [TorchScript super()]\u001B[39;00m\n\u001B[1;32m--> 268\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    269\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbn1(x)\n\u001B[0;32m    270\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrelu(x)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\torch\\nn\\modules\\conv.py:458\u001B[0m, in \u001B[0;36mConv2d.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    457\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 458\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_conv_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\trustworthyml\\lib\\site-packages\\torch\\nn\\modules\\conv.py:454\u001B[0m, in \u001B[0;36mConv2d._conv_forward\u001B[1;34m(self, input, weight, bias)\u001B[0m\n\u001B[0;32m    450\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mzeros\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m    451\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mconv2d(F\u001B[38;5;241m.\u001B[39mpad(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reversed_padding_repeated_twice, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode),\n\u001B[0;32m    452\u001B[0m                     weight, bias, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstride,\n\u001B[0;32m    453\u001B[0m                     _pair(\u001B[38;5;241m0\u001B[39m), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdilation, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroups)\n\u001B[1;32m--> 454\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv2d\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    455\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdilation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgroups\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mTypeError\u001B[0m: conv2d() received an invalid combination of arguments - got (str, Parameter, NoneType, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias = None, tuple of ints stride = 1, tuple of ints padding = 0, tuple of ints dilation = 1, int groups = 1)\n      didn't match because some of the arguments have invalid types: (!str!, !Parameter!, !NoneType!, !tuple of (int, int)!, !tuple of (int, int)!, !tuple of (int, int)!, !int!)\n * (Tensor input, Tensor weight, Tensor bias = None, tuple of ints stride = 1, str padding = \"valid\", tuple of ints dilation = 1, int groups = 1)\n      didn't match because some of the arguments have invalid types: (!str!, !Parameter!, !NoneType!, !tuple of (int, int)!, !tuple of (int, int)!, !tuple of (int, int)!, !int!)\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs/"
   ],
   "id": "be727aefe3b8073a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Test the model\n",
    "trainer.test()"
   ],
   "id": "778377282eab7dde",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "'''\n",
    "TODO:\n",
    "1. check tensorboard here in the cell directly \n",
    "2. evaluate the evaluate code as the checkpointing and reloading of that model is not as straightforward as I thought it would be \n",
    "'''"
   ],
   "id": "dadf7840d0a71ad9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 5. Saving model & Evaluating Results",
   "id": "f2232fa60d92bde6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def load_model_from_checkpoint(checkpoint_path):\n",
    "    # Load the checkpoint\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "\n",
    "    # Create a new instance of your model\n",
    "    finetuned_model = RobustResnet()  # Initialize with the required parameters\n",
    "\n",
    "    # Load the model weights from the checkpoint\n",
    "    finetuned_model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    finetuned_model.eval()\n",
    "    \n",
    "    return finetuned_model\n",
    "\n",
    "def evaluate_model(checkpoint_path, original_model, test_dataloader, device=DEVICE):\n",
    "    # Load the fine-tuned model from the checkpoint\n",
    "    fine_tuned_model = load_model_from_checkpoint(checkpoint_path)\n",
    "\n",
    "    # Move models to the appropriate device\n",
    "    original_model.to(device)\n",
    "    fine_tuned_model.to(device)\n",
    "\n",
    "    # Initialize accuracy metrics\n",
    "    original_acc = Accuracy()\n",
    "    fine_tuned_acc = Accuracy()\n",
    "\n",
    "    # Evaluate the original model\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            images, labels = batch\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Original model predictions\n",
    "            original_logits = original_model(images)\n",
    "            original_acc(original_logits, labels)\n",
    "\n",
    "            # Fine-tuned model predictions\n",
    "            fine_tuned_logits = fine_tuned_model(images)\n",
    "            fine_tuned_acc(fine_tuned_logits, labels)\n",
    "\n",
    "    # Calculate accuracies\n",
    "    original_accuracy = original_acc.compute()\n",
    "    fine_tuned_accuracy = fine_tuned_acc.compute()\n",
    "\n",
    "    print(f'Evaluation Original Model Accuracy: {original_accuracy:.4f}')\n",
    "    print(f'Evaluation Fine-Tuned Model Accuracy: {fine_tuned_accuracy:.4f}')\n",
    "\n",
    "    return original_accuracy, fine_tuned_accuracy"
   ],
   "id": "eed147733535b4b9",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
