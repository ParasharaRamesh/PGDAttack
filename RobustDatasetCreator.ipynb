{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "!pip install datasets\n",
    "!pip install -U \"huggingface_hub[cli]\""
   ],
   "id": "e865989fe04157d8"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-10T15:50:40.975119Z",
     "start_time": "2024-09-10T15:50:26.547805Z"
    }
   },
   "source": [
    "import os\n",
    "import random\n",
    "import uuid\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import ResNet50_Weights, resnet50\n",
    "import shutil\n",
    "import zipfile"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "id": "afe0bb621157c40e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T15:50:44.205553Z",
     "start_time": "2024-09-10T15:50:44.173065Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ResnetPGDAttacker:\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        The PGD attack on Resnet model.\n",
    "        :param model: The resnet model on which we perform the attack\n",
    "        :param dataloader: The dataloader loading the input data on which we perform the attack\n",
    "        '''\n",
    "        self.model = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "        # Nullify gradient for model params\n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def pgd_attack(self, image, label, eps, alpha, steps):\n",
    "        '''\n",
    "        Create adversarial images for given batch of images and labels\n",
    "\n",
    "        :param image: Batch of input images on which we perform the attack, size (BATCH_SIZE, 3, 224, 224)\n",
    "        :param label: Batch of input labels on which we perform the attack, size (BATCH_SIZE)\n",
    "        :return: Adversarial images for the given input images\n",
    "        '''\n",
    "        images = image.clone().detach().to(self.device)\n",
    "        adv_images = images.clone()\n",
    "        labels = label.clone().detach().to(self.device)\n",
    "\n",
    "        # Starting at a uniformly random point within the eps ball\n",
    "        random_noise = torch.zeros_like(adv_images).uniform_(-eps, eps)\n",
    "        adv_images = adv_images + random_noise\n",
    "\n",
    "        self.model.eval()\n",
    "        for _ in range(steps):\n",
    "            # Enable gradient tracking for adversarial images\n",
    "            adv_images.requires_grad = True\n",
    "\n",
    "            # Get model predictions and apply softmax\n",
    "            outputs = self.model(adv_images).softmax(1)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = self.loss_fn(outputs, labels)\n",
    "\n",
    "            # Compute gradient wrt images\n",
    "            grad = torch.autograd.grad(\n",
    "                loss, adv_images, retain_graph=False, create_graph=False\n",
    "            )[0]\n",
    "            adv_images = adv_images.detach()\n",
    "\n",
    "            # Gradient update\n",
    "            adv_images = adv_images + alpha * grad.sign()  # Update adversarial images using the sign of the gradient\n",
    "\n",
    "            # Projection step\n",
    "            # Clamping the adversarial images to ensure they are within the Lâˆž ball of eps radius of original image\n",
    "            adv_images = torch.clamp(adv_images, images - eps, images + eps)\n",
    "\n",
    "            adv_images = adv_images.detach()\n",
    "\n",
    "        return adv_images  # Return the generated adversarial images\n"
   ],
   "id": "f50664f92a518df4",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T16:19:52.627769Z",
     "start_time": "2024-09-10T16:19:30.201963Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class FineTuneDatasetGenerator:\n",
    "    def __init__(self, batch_size, batch_num, num_perturbations, local_save_path, zip_save_path,\n",
    "                 zip_every_n_batches=10, add_original=True):\n",
    "        self.batch_size = batch_size\n",
    "        self.batch_num = batch_num\n",
    "        self.num_perturbations = num_perturbations\n",
    "        self.local_save_path = local_save_path\n",
    "        self.zip_number = zip_every_n_batches\n",
    "        self.zip_save_path = zip_save_path\n",
    "        self.zip_buffer = []\n",
    "        self.add_original = add_original\n",
    "\n",
    "        # Create the save directory if it doesn't exist\n",
    "        os.makedirs(self.local_save_path, exist_ok=True)\n",
    "        os.makedirs(self.zip_save_path, exist_ok=True)\n",
    "\n",
    "        weights = ResNet50_Weights.DEFAULT\n",
    "        self.resnet_transform = weights.transforms()  # PIL -> tensor\n",
    "\n",
    "        self.pgd_attacker = ResnetPGDAttacker()\n",
    "\n",
    "        self.ds = load_dataset(\"ILSVRC/imagenet-1k\", split=\"train\", streaming=True, trust_remote_code=True)\n",
    "        self.ds = self.ds.shuffle()\n",
    "        self.ds = self.ds.filter(lambda example: example['image'].mode == 'RGB')\n",
    "        self.ds = self.ds.take(self.batch_num * self.batch_size)\n",
    "        self.ds = self.ds.map(self.preprocess_img)\n",
    "        self.dataloader = DataLoader(self.ds, batch_size=self.batch_size)\n",
    "        print(f\"Fine Tune Dataset Generator has been initialized. save path is {self.local_save_path}, zips path is {self.zip_save_path}\")\n",
    "\n",
    "    def preprocess_img(self, example):\n",
    "        example['image'] = self.resnet_transform(example['image'])\n",
    "        return example\n",
    "    \n",
    "    def add_batch_to_buffer(self, images, labels):\n",
    "        for image, label in zip(images, labels):\n",
    "            self.zip_buffer.append((image, label))\n",
    "        return\n",
    "\n",
    "\n",
    "    def generate(self):\n",
    "        for i, batch in enumerate(tqdm(self.dataloader, total=self.batch_num)):\n",
    "            if i % self.zip_number == 0 and len(self.zip_buffer) > 0:\n",
    "                self.save_files_in_buffer_and_zip()\n",
    "\n",
    "            images, labels = batch[\"image\"], batch[\"label\"]\n",
    "            if self.add_original:\n",
    "                self.add_batch_to_buffer(images, labels)\n",
    "\n",
    "            #do perturbations and then add \n",
    "            for _ in range(self.num_perturbations):\n",
    "                # Generate random parameters for PGD attack\n",
    "                random_eps = random.uniform(0.01, 0.3)\n",
    "                random_alpha = random.uniform(0.01, 0.07)\n",
    "                random_steps = random.randint(15, 20)\n",
    "\n",
    "                # Perform the PGD attack\n",
    "                perturbed_images = self.pgd_attacker.pgd_attack(images,\n",
    "                                                                labels,\n",
    "                                                                eps=random_eps,\n",
    "                                                                alpha=random_alpha,\n",
    "                                                                steps=random_steps)\n",
    "                self.add_batch_to_buffer(perturbed_images, labels)\n",
    "\n",
    "        if len(self.zip_buffer) > 0:\n",
    "            # for the last batch\n",
    "            self.save_files_in_buffer_and_zip()\n",
    "\n",
    "\n",
    "    def zip_folder(self):\n",
    "        \"\"\"Zips the contents of a folder into a zip file.\"\"\"\n",
    "        id = str(uuid.uuid4()) + \".zip\"\n",
    "        zip_path = os.path.join(self.zip_save_path, id)\n",
    "        \n",
    "        print(f\"Zipping current folder {self.local_save_path} with {len(os.listdir(self.local_save_path))} items to {zip_path}\")\n",
    "        \n",
    "        with zipfile.ZipFile(zip_path, \"w\") as zip_file:\n",
    "            for root, dirs, files in os.walk(self.local_save_path):\n",
    "                for file in files:\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    zip_file.write(file_path)\n",
    "        print(f\"Created zip in {zip_path}\")\n",
    "        \n",
    "    def remove_files_in_directory(self, directory):\n",
    "        # Iterate over all files and directories in the specified directory\n",
    "        for filename in os.listdir(directory):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            try:\n",
    "                if os.path.isfile(file_path):\n",
    "                    os.remove(file_path)\n",
    "                elif os.path.isdir(file_path):\n",
    "                    shutil.rmtree(file_path)\n",
    "            except Exception as e:\n",
    "                print(f\"Error removing {file_path}: {e}\")\n",
    "\n",
    "    def save_files_in_buffer_and_zip(self):\n",
    "        for data in tqdm(self.zip_buffer, desc=\"Saving batch as file in temp location\"):\n",
    "            image, label = data\n",
    "            img_id = str(uuid.uuid4())\n",
    "            save_file = os.path.join(self.local_save_path, f\"class_{label}_img_{img_id}.pt\")\n",
    "            torch.save(image, save_file)\n",
    "\n",
    "        #creating zip and copying zip to zip_save_path\n",
    "        self.zip_folder()\n",
    "\n",
    "        #deleting all files from local_save_path\n",
    "        self.remove_files_in_directory(self.local_save_path)\n",
    "\n",
    "        #clearing buffer for next time\n",
    "        self.zip_buffer.clear()\n",
    "        \n",
    "\n",
    "#Local test:\n",
    "generator = FineTuneDatasetGenerator(\n",
    "    add_original=False,\n",
    "    batch_size=2, \n",
    "    batch_num=6, \n",
    "    num_perturbations=2, \n",
    "    local_save_path= \"./data_gen_test\",\n",
    "    zip_save_path=\"./data_gen_zips_test\",\n",
    "    zip_every_n_batches=2\n",
    ")\n",
    "generator.generate()\n"
   ],
   "id": "f8b5e785127b8830",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine Tune Dataset Generator has been initialized. save path is ./data_gen_test, zips path is ./data_gen_zips_test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 2/6 [00:13<00:22,  5.57s/it]\n",
      "Saving batch as file in temp location: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 230.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zipping current folder ./data_gen_test with 8 items to ./data_gen_zips_test\\52816ff0-f3f3-4f97-ab3e-4c5c7dab17aa.zip\n",
      "Created zip in ./data_gen_zips_test\\52816ff0-f3f3-4f97-ab3e-4c5c7dab17aa.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 4/6 [00:15<00:05,  2.50s/it]\n",
      "Saving batch as file in temp location: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 266.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zipping current folder ./data_gen_test with 8 items to ./data_gen_zips_test\\ad74644d-bf1c-47de-b3d6-179a66f0c4f8.zip\n",
      "Created zip in ./data_gen_zips_test\\ad74644d-bf1c-47de-b3d6-179a66f0c4f8.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:17<00:00,  2.84s/it]\n",
      "Saving batch as file in temp location: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 397.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zipping current folder ./data_gen_test with 8 items to ./data_gen_zips_test\\ed7a6850-2a10-49d1-9c1f-0782305da764.zip\n",
      "Created zip in ./data_gen_zips_test\\ed7a6850-2a10-49d1-9c1f-0782305da764.zip\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "local_path = \"/content/dataset\"\n",
    "base_path = \"/content/drive/MyDrive/trustworthyml\"\n",
    "zips_path = os.path.join(base_path, \"zips\")\n",
    "\n",
    "generator = FineTuneDatasetGenerator(\n",
    "    batch_size=16, \n",
    "    batch_num=1000, \n",
    "    num_perturbations=3, \n",
    "    local_save_path= local_path,\n",
    "    zip_save_path=zips_path,\n",
    "    zip_every_n_batches=4\n",
    ")\n",
    "generator.generate()"
   ],
   "id": "8015057aaa037f9e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Unzipping code",
   "id": "b2204767fac6f33d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3335dff561e7fbee"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
